{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe41752",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "1),for continuous features it becmes \"Regretion_Problem\" \n",
    "\n",
    "2)whenever you have your output/dependent feature is haveing\n",
    "fix no.of \"categiries\" then that becomes a \"Classification_Problem\"\n",
    "it it has more than 2 categories then it becomes a \n",
    "\"Multy_Class_Classification\"\n",
    "\n",
    "Gradient decent \" MSE \" \"mean_squred Erorr\" formula\n",
    "\n",
    "\n",
    "\"Cost function\" :=\n",
    "    \n",
    "                  1     m \n",
    "--> J(θ0 , θ1) = -----  ∑ (hθ(xi) - yi)² [:. hθ(x)=θ0 + θ1x]\n",
    "                  2m   i=1               [:. hθ(xi)=^y ]\n",
    "    \n",
    "   =========================================================== \n",
    "        \n",
    "        ax² + bx + c =0 \n",
    "                           1    m\n",
    "                         -----  ∑ (^y-y)²  ==> \"quadratic equation\"\n",
    "                           2m  i=1                                                      \n",
    "                                                 \n",
    "   ===================================================== \n",
    "\n",
    "@Gradient Desent algorithem uses Convergence Algorithem:=\n",
    "   Ans :=\n",
    "    convergence algrm says repeat until \"Convergence\"\n",
    "    \n",
    "  now how this \"θ1\" Updation happens ?\n",
    "    \n",
    "                                                 \n",
    "           {  \n",
    "                            d(J(θold))\n",
    "            θnew = θold - α -----------  ==>coefficient updation  \n",
    "                               dθold\n",
    "           }  \n",
    "            \n",
    "############################################################### \n",
    " \n",
    "    \n",
    "    \n",
    "    hθ(xi) = θ1x1 + θ2x2 + θ3x3 + θ4x4 + θ0  ---> this is caled as\n",
    "                              \"Multy_Linear_Regretion\"\n",
    "        \n",
    "========================================================\n",
    "Gradient decent \" MAE \"  \n",
    "\n",
    "if there is \"Outliers\" in 'feature' wwhich method we choose?\n",
    "Ans := \n",
    "    we choose \"MEAN ABSOLUTE ERORR\" \n",
    "\n",
    "if there is \"Outliers\" in \"MSE\"\" mean square erorr\" :=\n",
    "    \n",
    "   we use another type of \"gradient decent\" that is \n",
    "   \"MEAN ABSOLUTE ERORR\"   \n",
    "                 1    m  \n",
    "  J(θ0 , θ1) = -----  ∑ |^y- y|    [ :. ^y = hθ(xi) ]                                             \n",
    "                 m   i=1    \n",
    "#########################################################\n",
    "\n",
    "Gradient decent \" RMSE \"  :=\n",
    "    RMSE is nothing but \"Root Mean Square Erorr\"\n",
    "    \n",
    "   Formula   \n",
    "                   \n",
    "                                    Σ(xi – yi)²\n",
    "                      RMSE =     √ ----------\n",
    "                                        n      \n",
    "\n",
    "#####################################################\n",
    "\"R²\" and \"Adjusted_R²\"\n",
    "     \"R²\" :=\n",
    "           R² is the \"performance metrix\" to check how good this\n",
    "           is specific model is                                      \n",
    "                                    \n",
    "             Ss_Res            [:.Ss_Res   = \"some_of_resideal\"]             \n",
    "   R² = 1 - ---------          [:.Ss_total = \"some_of_total\"   ]                       \n",
    "             Ss_total\n",
    "                        \n",
    "               some_of_resideal = ∑(yi - y^)²\n",
    "               some_of_total    = ∑(yi - ȳ)²    [:. ȳ =  \"Mean\"]   \n",
    "    \n",
    "    \n",
    "Can i get  \"R²\" is nagative value ?\n",
    "ans :=\n",
    "    our \"best fit line\" is  worse than our specific \"mean\"(\"ȳ\")/\"Average\"\n",
    "    line then i may get \"nagative value\", if i am getting near to one \n",
    "    i am getting \"best fit line\"\n",
    "    \n",
    "\"Adjusted_R²\" is 0.87  \n",
    "                                                 \n",
    "                     (1 - R²)(N - 1)               \n",
    "    Adjusted_R² = 1 - -----------------  [:.p = features or no.of predictores ]\n",
    "                       N - P - 1         [:.N = no.of datapoits ]\n",
    "        \n",
    "#########################################################################\n",
    "\"Ridge regration\" (L2_regularisation):=\n",
    "    \n",
    "        \n",
    "   Cost_function and \"Ridge regration\"  :=\n",
    "    \n",
    "                     1    m \n",
    "      J(θ0 , θ1) = -----  ∑ (hθ(xi) - yi)² + λ (slope)²\n",
    "                     2m  i=1  \n",
    "        \n",
    "        [:. λ = hyper parameter ]\n",
    "        [:. Slope = coffiescent (\"θ1\")]\n",
    "        \n",
    "note := 1.when \"λ\"value is increasing \"slope\" is \"decreasing\" \n",
    "        2.\"global minima\"getting pussed near to 0, but it is not\n",
    "        exactly 0\n",
    "why we are doing this ?\n",
    "        we doing this for to reduce \"overfitting\", bedcoze we will\n",
    "        be getting the proper line/\"best fit line\",  we need to get  \n",
    "        a \"better line\" then previous \"best bit line\" so that \n",
    "        its works good for the \"Training_data\" and \"Test_data\".\n",
    "        means the \"Train_Error\" and \"Test_error\" is less \n",
    "        \n",
    "        \n",
    "2. Lasso Regration (\"L1 regularisation\"):=\n",
    "    \n",
    "\n",
    "\n",
    "                     1    m \n",
    "      J(θ0 , θ1) = -----  ∑ (hθ(xi) - yi)² + λ |slope|\n",
    "                    2m   i=1            \n",
    "                                        [:. λ = hyper parameter ]\n",
    "                                        [:. slope = coffiescent]\n",
    "                \n",
    "    ** final Conclussion  ** \n",
    "    \n",
    "      Ridge := \"Reduce Overfitting\" \n",
    "      Lasso := \"Feature selection\"\n",
    "        \n",
    "so in \"Losso_Rigretion\" where ever the \"Coefficient\"/\"Slop\" near\n",
    "to 0 or less, \"Losso\" is going to \"bring them\" to the 0 \"Slop\"/\"Coefficient\"\n",
    "\n",
    "###############################################################\n",
    " \"Elastic_Net\" Regration :=\n",
    "        \n",
    "    if we Combine Redge and Losso_regration it will become an \n",
    "    \"Elastic_Net\" Regration\n",
    "    \n",
    "                   1    m \n",
    "    J(θ0 , θ1) = -----  ∑ (hθ(xi) - yi)² + λ1 (slope)² + λ2|slope|\n",
    "                  2m   i=1           \n",
    "                                        [:. λ = hyper parameter ]\n",
    "                                        [:. slope = coffiescent]\n",
    "                \n",
    "    \"λ1\" and \"λ2\" are saperate values.\n",
    "            \n",
    "in this Regration we are using both componants so here we are also\n",
    "redusing \"Over_fitting\" and we also performing \"Feature Selection\"\n",
    "\n",
    "###########################################################\n",
    "Logistic_Regration : (Classification problem)\n",
    "    \n",
    "    \n",
    "            hθ(x) for \"logistic regration\"\n",
    "        will be a function of ---------> hθ(x) = g( θ0 + θ1x1 )\n",
    "        \n",
    "       --> hθ(x) = g( θ0 + θ1x1 )  :.θ0 + θ1x1 for 2 \"cordinates\"\n",
    "                                     if i have more \"cordinates\"\n",
    "                                     i can write this as \n",
    "                           hθ(x) = θ0 + θ1x1 +θ2x2+θ3x3 +..+θnXn\n",
    "                    \n",
    "            here what this \"g \" ?\n",
    "                         \n",
    "       --> let say Z = θ0 + θ1x1 \n",
    "    \n",
    "                       :. g is a function on \"Z\"\n",
    "        hθ(x) =g(Z)  [:. g is a \"activation function\" ]\n",
    "        \n",
    "---------------------------------------------------------------------        \n",
    "        \n",
    "                 1\n",
    "     hθ(x) = ---------  this \"activation function\" does 2 things\n",
    "              1 + (e)-z   \n",
    "        \n",
    "if i talk about \"hypothesis testing\" my hθ(x) is nothing but\n",
    "\n",
    " Logistic regration hypothesis:=\n",
    "             \n",
    "             Z = θ0 + θ1x1 \n",
    "                \n",
    "                        1            }\n",
    "        hθ(x) =   ---------------    } this entire Equation is ranging \n",
    "                  1 + e-(θ0+θ1x1)    }  b/w [0 - 1]\n",
    "            \n",
    "    if \"intersept\" is θ = 0 and my \"hypothesis\" will be \n",
    "                 1\n",
    "    hθ(x) = ------------ \n",
    "             1 + e-(θ1x)  ---> can i use this \"hypothesis\"\n",
    "                              in my \"Cost_function\"\n",
    "            \n",
    "            \n",
    "1. NON Convex function\n",
    "2. Convex function            \n",
    "            \n",
    "_______________________________________________________________\n",
    "\n",
    "         |   -log(hθ(x))  use this \"cost_function\" if your value  y=1\n",
    "  j(θ1)= | \n",
    "         |   -log(1-hθ(x)) use this \"cost_function\" if your value y=0  \n",
    "\n",
    "    internally i can write this as \n",
    "\n",
    "    J(θ0,θ1) = -y log(hθ(xi)) - (1-y)log(1 - hθ(x))  \n",
    "    \n",
    "    if y = 1 i get = -log(hθ(x)) this equation\n",
    "    if y = 0 i get = -log(1-hθ(x)) this equation\n",
    "            \n",
    "----------------------------------------------------------\n",
    "        \n",
    "\n",
    "  \"Cost_function\" for \"Logistic_Regration\" :=\n",
    "\n",
    "\"cost_function\":=\n",
    "                   1    m \n",
    "    J(θ0 , θ1) = -----  ∑ (hθ(xi) - yi)²\n",
    "                  2m   i=1\n",
    "        \n",
    "         - 1    m \n",
    "J( θ1) = -----  ∑ [yi log(hθ(xi)) + (1-yi)log(1-hθ(xi))]\n",
    "          2m   i=1\n",
    "    \n",
    "    \n",
    "-------------------------------------------------\n",
    "\n",
    "for Classification Performance Matrix is\n",
    "\n",
    "                          TP + TN          3+1        4 \n",
    "Accuracy of the model = ------------  = --------- = ---- = 57%\n",
    "                         TP+FP+FN+TN     3+2+1+1      7\n",
    "    \n",
    "1 .\"Precission\" :=\n",
    "    \n",
    "    out of all \"predicted output\" how many of them are been predicted \n",
    "correctly or as \"TP\" are there\n",
    "formula:=\n",
    "    \n",
    "        TP\n",
    "    = -------\n",
    "       TP+FP            \n",
    "    \n",
    "    \n",
    "2 . \"Recall\" :=     \n",
    "    \n",
    "   out of all the \"actuall values\" how many them are \n",
    "were as \"TP\"   this also called as \"TP rate\"  \n",
    "\n",
    "        TP\n",
    "    = -------\n",
    "       TP+FN\n",
    "        \n",
    "3.\"F_Score\" /\"F_Beta\" Score  :=\n",
    "    we basically use this \"F_Score\" when we have to concider both the\n",
    "    resons \"FP\" and \"FN\"\n",
    "        \n",
    "        \n",
    "                                     precision * recall\n",
    "\"F_Beta\" Score formula == (1 + β²) -----------------------\n",
    "                                   β² *[precision+recall]\n",
    "    \n",
    "    lets say \"FP\" is more imp then \"FN\"(FP>>FN)  \n",
    "    then i have to reduse my   \"β\" \n",
    "    \n",
    "    so \"β\" = 0.5\n",
    "    \n",
    "                    precision * recall\n",
    "        = (1 + 0.25)----------------------- this equation for when my\n",
    "                    [0.25][precision+recall]  \"FP\" is more IMP then \"FN\"\n",
    "                                              (FP>>FN)\n",
    "                \n",
    "    lets say \"FN\" is more imp then \"FP\"(FP<<FN) so \"increase\"\n",
    "    the β = 2\n",
    "    \n",
    "                        precision * recall     \n",
    "              = (1 + 4)----------------------this equation for when my\n",
    "                       [4][precision+recall]  \"FN\" is more imp then \"FP\"\n",
    "                                              (FP<<FN)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
