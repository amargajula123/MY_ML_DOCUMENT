{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1d7a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "Agenda :=\n",
    "    \n",
    "\n",
    "1. Gradiant Boost\n",
    "2. XGboost (Xtream Gradiant Boost)\n",
    "*\n",
    "Unsupervised ML -->\n",
    "\n",
    "                   --> K means clustring }\n",
    "                   --> Hicrarichal mean  }-we calculate this 3 thing\n",
    "                   --> DB scane          } with \"sillhouites score\" \n",
    "            \n",
    "for \"feature selection\" in ml we use\n",
    "                      \"correlation\" \n",
    "                    \"pearson correlation\" \n",
    "                      \"+ve corr\" & \"-ve corr\"\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f5d363",
   "metadata": {},
   "outputs": [],
   "source": [
    "Interview \n",
    "\n",
    "{\"Project\"  \"Theoretical\"   life cycle of \"Datasciense Project\"}\n",
    "\n",
    "\n",
    "where do you collect the data ?\n",
    "Ans:\n",
    "    i did some srapping i went some of the 3 rd party \"apis\"\n",
    "    and i try to  create a data set from this data set i \n",
    "    create more derive dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6c073b",
   "metadata": {},
   "source": [
    "# Gradiant Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bcb9006",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m13\u001b[0m\n\u001b[1;33m    Exp       Degree  |     Salary   resudel1  resudual2 resudial3\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "@Gradiant Boost :=\n",
    "    This is the \"Boosting technic\", Gradiant boost is used for\n",
    "    \"Classification\" problem and  \"Regration\" problem,\n",
    "    \n",
    "    by defult\n",
    "    Gradiant uses 100 \"Decision trees\" like random forest\n",
    "    \n",
    "let say i have some features \n",
    "\n",
    "independent fetures         dependent feature\n",
    "     ------|------         --|--\n",
    "     |           |           |                 letssay\n",
    "    Exp       Degree  |     Salary   resudel1  resudual2 resudial3 \n",
    "    ------------------|-----------\n",
    "    2          BE            50k      -25       -23        -13.5\n",
    "    3         MASTERS        70k      -5        -3         -3.5    \n",
    "    5         MASTERS        80k       5         3          3.5\n",
    "    6          PHD          100k       25        23         13.5\n",
    "    \n",
    "    \n",
    "    50-75=-25\n",
    "    70-75=-5\n",
    "    80-75=5\n",
    "    100-75=25\n",
    "    \n",
    "step :1 initilize the \"Model\" with \"Constant value\".\n",
    "    \n",
    "    what happens in \"Base Model\" ?\n",
    "    \n",
    "    we create a \"Base Model\"  this \"Base Model\" will only Going to \n",
    "    give the \"same Output\".\n",
    "    \n",
    "    NOTE :=\n",
    "        if it is a \"Base Model\" also called as  \"Constant Model\"\n",
    "        b-coz it give the same \"Output\"\n",
    "    \n",
    "        \n",
    "How do we define/findout the \"Base Model\" ?\n",
    "\n",
    "in \"Boosting\" we create \"Sequantil Modles\" or \"Sequantil Decission_Trees\"\n",
    "\n",
    "        |--->\"Base model\"  \n",
    "        |               \n",
    "    |--------|      |----------|    |--------|                |------|\n",
    "    |        |--->  |   DT     |    | DT     |                | DT   |\n",
    "    |        |      |   1      |--->|   2    |- - - - - - - ->|  100 |\n",
    "    |--------|      |----------|    |--------|                |------|\n",
    "  \"Base model\"           / \\            / \\                     / \\   \n",
    "    \"Avg\" = 75k         /   \\          /   \\                   /   \\        \n",
    "                       /     \\        /     \\                 /     \\\n",
    "                      🔘    🔘      🔘    🔘              🔘     🔘\n",
    "      \n",
    "   \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "               50+70+80+100\n",
    "Base model =  -------------- = 75k\n",
    "                     4 \n",
    "    \n",
    "* We take the \"Avarage\" of \"Output feature\" to create a \"Base model\" \n",
    "\n",
    "* After my \"Base model\" iam getting i need to look the difference B/w\n",
    "  my \"Base model\" and \"output feature\" then i will get the \"Resudel1\" \n",
    "  values which is nothing but my \"Error\" \n",
    "\n",
    "* after getting my \"resudel1\"(Error) with respect to my \"Base Model\"\n",
    "  then we create another \"Decision_Tree\" but input features for this\n",
    "  \"Decision_Tree\" will be \"different\" like we take \"independet\"features\n",
    "  but my \"Output\" feature that i will take is that \"Resudel1\" which \n",
    "  comes with respect by the \"Base Model\".\n",
    "    \n",
    "Why we are taking psudo \"Resudules\" ?\n",
    "Ans:=\n",
    "    we need to \"Reduse\" the \"Resudels\". if i reduse the \"Resudules\" ill be\n",
    "    calculate how the \"Dependent feature\" will be\n",
    "\n",
    "* if i take \"independent features\" and \"Resudule1\" in my next \"Decision_Tree\"\n",
    "  i will be getting \"Resudule2\"(\"Error\") with some \"near\" values\n",
    "  \n",
    "* So here if i pass any \"New record\" the \"output\" will be near to \"Resudual1\"\n",
    "  only, b-coz we are getting the \"outputs\"Rsl2 with \"independent features\" and\n",
    "  \"Resudul1\"(Error) are near to \"Rsudeual1\"\n",
    "    \n",
    "\n",
    "  \n",
    "Lets say if iam getting \"new record\"(testdata) as 3.5 as my \"input feature\"\n",
    "what is the output will get means Obviously near to  \"Rsudeual1\"(Error)\n",
    "\n",
    "\n",
    " \"Base model\" or \"Constant\" Model will give u Out = 75 every time\n",
    "    \n",
    "if we pass the \"new Record\" to \"base model\" it give 75 only its \"Constant\" Mdl\n",
    "then it goes to \"Decision_Tree\" in \"Decision_Tree\" 2 we will add the \n",
    "\"Decision_Tree\" 1 value\n",
    "\n",
    "\n",
    " 3 & 5 iam get the \"Resudules2\" as  -3  \n",
    "    \n",
    "    if i calculate 75+(-3)= 72 its near to 70k yes \"prediction\" is good, but\n",
    " it leads to \"Overfitting\". becoz both values are very much 'similar'\n",
    "\n",
    "in this order to \"Reduse\" the \"overfitting\" we use \"Learnig rate\"(α)\n",
    "\n",
    "   [:. α  \"Learnig rate\" will varie/ranging b/w 0 to 1 like 0.5 ]\n",
    "\n",
    " \n",
    "          75 + α (-3) ==>[:.α = learing rate=0.5]\n",
    "    \n",
    "          75+(0.5)(-3) = 75-1.5 = 73.5\n",
    "        \n",
    "Like so if i fitting my \"independer feature\" and along with \"Resudual1\"\n",
    "as my Output i will be getting \"Resudual2\" which is Obviosly \"near\" to \n",
    "\"Resudual1\".\n",
    "\n",
    "if i Combine \"Decision_Trees\" i will be getting my \"Resudules3\"\n",
    "        \n",
    "        like \"Resudual3\" \n",
    "        \n",
    "        lets consider  α =  \"learning rate\" = 0.5\n",
    "        \n",
    "75+(0.5)(-23) = 63.5  --> 50 - 63.5 = -13.5\n",
    "75+(0.5)(-3)  = 73.5  --> 70 - 73.5 = -3.5\n",
    "75+(0.5)(3)   = 76.5  --> 80 - 76.5 = 3.5\n",
    "75+(0.5)(23)  = 86.5  --> 100- 86.5 = 13.5\n",
    "\n",
    "Note :-\n",
    "    in Gradient boost also there are 1000 \"Decision_Trees\" by defult\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "                \n",
    "        \n",
    "my final equation like \n",
    "\n",
    "\"Base Model\" = h0(x)\n",
    "\n",
    "Decision tree_1 = α1 h1(x1)\n",
    "Decision tree_2 = α2 h2(x2)\n",
    "                    .\n",
    "                    .  \n",
    "\"Decision tree_n\" = αn hn(xn)\n",
    "\n",
    "f(n) = h0(x) + α1 h1(x1) + α2 h2(x2) + α3 h3(x3)+...+.αn hn(xn)\n",
    "                      ||\n",
    "                      ||\n",
    "f(n) = h0(x) + α1 h1(DT1) + α2 h2(DT2) + α3 h1(DT3)+...+.αn hn(DTn)\n",
    "\n",
    "[:.α will help you to redusing the \"overftting\"]\n",
    "\n",
    ":. α = here \"learning rate\" it is a kind of \"hyper parameter\"\n",
    "    \"internally\" it will try te seleted\n",
    "\n",
    "Assumtions :=\n",
    "    each and every model will we be a weak  learner over here if\n",
    "    i combine all together \"Sequentially\" it will be a \"strong learner\"\n",
    "    \n",
    "how this \"Gradiant_Boosting\" will make strong learner ?\n",
    "ans :=\n",
    "    each and every \"Decision_tree\" will get applyed to \"learnning rate\"(α)\n",
    "    \n",
    "    b-coz of \"learnning rate\"(α) it \"reduses\" the \"Overfitting\", thats\n",
    "    how \"Decision_Trees\" make as \"strong learner\". \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4063a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.5"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "75+(0.5)(-23) = 63.5  --> 50 - 63.5 = -13.5\n",
    "75+(0.5)(-3)  = 73.5  --> 70 - 73.5 = -3.5\n",
    "75+(0.5)(3)   = 76.5  --> 80 - 76.5 = 3.5\n",
    "75+(0.5)(23)  = 86.5  --> 100- 86.5 = 13.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f86d336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12.5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "50-62.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "155f758d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62.5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "75+(0.5)*(-25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fd69f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e429a3cb",
   "metadata": {},
   "source": [
    "# XGboost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9aad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBOOST : https://www.youtube.com/watch?v=gPciUPwWJQQ\n",
    "\n",
    "@XGboost Clasifier :=\n",
    "    :=\n",
    "        in \"XG_boost\" the \"Decision_Tree\" is Constracted the way its\n",
    "        its constracted is different when comapare to \"Gradient_boosting\"\n",
    "        XGboost also known as \"Xstream Gradient_boosting\".\n",
    "        \n",
    "    \n",
    "    salary          credit       Approval   Residal\n",
    "    \n",
    "     <=50K            B              0       -0.5           \n",
    "     <=50K            G              1        0.5\n",
    "     <=50K            G              1        0.5\n",
    "     > 50K            B              0       -0.5 \n",
    "     > 50k            G              1        0.5\n",
    "     > 50K            N              1        0.5\n",
    "     <=50K            N              0       -0.5 \n",
    "        \n",
    "        \n",
    "        \n",
    "How the \"Decision_Tree\" is \"Constracted\" in \"XG_boost\" Classifier ?\n",
    "\n",
    "\n",
    "      B = bad\n",
    "      G = good\n",
    "      N = normal\n",
    "    Aproval 1,0 = if you get credit cart its 1 not means 0\n",
    "    \n",
    "step :1 := in \"XG_boost\" Classifier Constact \"tree\" with root\n",
    "        \"Constract Base Model\". \n",
    "        \n",
    "        \"ouput\" of this \"Base model\" is by defult = 0.5  why 0.5\n",
    "        means we have only 2 outputs in \"classification\"\n",
    "        so the \"avarage\" of those (0 either 1) will be like 0+1/2 = 0.5\n",
    "        \n",
    "                     |---| \n",
    "                     |---|  \"Base model\" \n",
    "                      🔘 \"Decission_tree\"   \n",
    "                      / \\    \n",
    "                     /   \\        \n",
    "                    /     \\\n",
    "                  🔘      🔘\n",
    "                    o/p=0.5\n",
    "                     \n",
    "        Note := we can not take \"Meadian\" b-coz our dataset should be\n",
    "                in any way\n",
    "                \n",
    "                    \n",
    "    like \"Resiudual\"(Error) getting 0-0.5  = -0.5\n",
    "                                     1-0.5 = 0.5\n",
    "        \n",
    "step : 2 :=\n",
    "        Constract \"Tree\" with \"Root node\" \n",
    "        \n",
    "here we take \"independent variables\" as \"salary\"and \"credit\"\n",
    "and 'residal' as my \"output\" over here \n",
    "        \n",
    "    Note : Alway \"remember\" in \"XG_boost\" when you \"constract\" a \"Tree\" \n",
    "        it should be a \"Binary classifier\" even if your feature having\n",
    "        3,4..n \"categories\" then also it should be \"2/Binary classifier\"\n",
    "        or the \"leafnode\" will actually 2/Binary\n",
    "        \n",
    "     [-0.5,0.5,0.5,-0.5,0.5,0.5,-0.5]\n",
    "                  salary\n",
    "          <=50K      |     >50k\n",
    "            |----------------|\n",
    "            |                | \n",
    "            |                |\n",
    "    [-0.5,0.5,0.5,-0.5]  [-0.5,0.5,0.5] \"split will happen until find the\" \n",
    "                                         \"leaf node \"\n",
    "    \n",
    "    \n",
    "step :3 :=\n",
    "        \n",
    "after this in constacting a \"Decision_Tree\" we calculate\n",
    "the \"similarity weight\", this \"siilarity weight\" will help\n",
    "us to give the \"output\"   \n",
    "\n",
    "                       ∑(Residual)²\n",
    "\"Similarity weight\" = ----------------\n",
    "                       ∑P_r(1 - P_r)+λ\n",
    "    \n",
    "      [:. λ is kinda \"hyper parameter\" ]\n",
    "        \n",
    "@similarity weight for (<=50K) :=   \n",
    "\n",
    "    ∑(Residual)² = [(-0.5)+0.5+0.5+(-0.5)]² = 0\n",
    "    \n",
    "    \"P_r\" is comming from your \"Base model\" that is 0.5\n",
    "    \n",
    "    ∑P_r(1-P_r)+λ = [0.5(1-0.5)+how many reiduals over here] => 1-0.5 =0.5\n",
    "    ∑P_r(1-P_r)+λ = [0.5(0.5)+0.5(0.5)+0.5(0.5)+0.5(0.5)] = 1\n",
    "\n",
    "\"Similarity weight\" for (<=50K) = 0/1 = 0\n",
    "    \n",
    "@similarity weight for (>50K) :=\n",
    "    \n",
    "    ∑(Residual)² = [(-0.5)+0.5+0.5]² = 0.25\n",
    "    \n",
    "    P_r is comming from your \"base model\" that is 0.5\n",
    "    \n",
    "                                                  [:. 1-0.5 =0.5]\n",
    "    ∑P_r(1-P_r)+λ = [0.5(1-0.5)+how many reiduals(error) over here]\n",
    "    ∑P_r(1-P_r)+λ = [0.5(0.5)+0.5(0.5)+0.5(0.5)] = 0.75\n",
    "    \n",
    "\"Similarity weight\" for >50K  = 0.25/0.75 = 0.33333\n",
    "\n",
    "    \n",
    "@similarity weight for \"base model\" of \"root node\" :=\n",
    "    \n",
    "    ∑(Residual)² = [-0.5+0.5+0.5+(-0.5)+0.5+0.5+(-0.5)]² = 0.25\n",
    "    \n",
    "    \n",
    "    P_r is comming from your \"base model\" that is 0.5\n",
    "    \n",
    "    ∑P_r(1-P_r)+λ = [0.5(1-0.5)+how many no.of reiduals(error) over here]\n",
    "    ∑P_r(1-P_r)+λ = [0.5(0.5)+0.5(0.5)+0.5(0.5)+\n",
    "                    0.5(0.5)+0.5(0.5)+0.5(0.5)+\n",
    "                    0.5(0.5)] = 1.75\n",
    "    \n",
    "\"Similarity weight\" for \"root node\" = 0.25/1.75 = 0.142\n",
    "\n",
    "                    salary SW = 0.142\n",
    "                      🔘   \n",
    "                      / \\   >50K\n",
    "               <=50K /   \\        \n",
    "                    /     \\\n",
    "           SW = 0 🔘      🔘  SW = 0.33333\n",
    "                \n",
    "                Now we need to calculate the \"Information Gain\"\n",
    "                \n",
    "    \" Information Gaine\" = 0+0.3333-0.142 = 0.188\n",
    "                \n",
    "\n",
    "Why we are calculating the \"siilarity weight\" ?\n",
    "\n",
    "step :4:= to Know what is the \"Gain\" so we need to calculate\n",
    "        \"similarity weight\" \n",
    "        \n",
    "    we calculate \"siilarity weight\" is for to get the \"Gain\"\n",
    "        \n",
    "  [:.\"Child_1 node\" + \"Child_2 node\" - \"Rootnode\" = Information Gain]\n",
    "        \n",
    "        \"Information Gaine\" = 0 + 0.33 - 0.142 = 0.188\n",
    "        \n",
    "Why we are calculating Gain ?\n",
    "\n",
    "Ans := we know. to select the features \"Information Gaine\" will helps you\n",
    "      see here we select for \"Spilting\" Creadit Feature also we can start\n",
    "     but what i am doing is that we spilt Next Tree(Decision_Tree) based\n",
    "    on that we calculate the Information Gain what which ever \"Information\"\n",
    "    \"Gain\" is \"high\" that we select\n",
    "    \n",
    "    Note : so here for Selecting features \"Information Gain\" is helps us\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "Let say with respect to \"Root Node\" if we take \"credit\" and \"Gain\" is\n",
    "like 0.234 so \n",
    "          Salry Gain < Credit Gain => 0.188 < 0.234\n",
    "     so we need to take the high \"Gain\" to do the \"Best Spilt\"\n",
    "    thats why we are calculating the \"Similarity weights\" and \"Gain\"\n",
    "        \n",
    "\n",
    "Split for another feature that is \"Creadit\"         \n",
    "        \n",
    "        do the split of \"credit\" feature\n",
    "        \n",
    "        [-0.5,0.5,0.5,-0.5,0.5,0.5,-0.5]\n",
    "                  salary\n",
    "                 <=50K      |     >50k\n",
    "                   |----------------|\n",
    "                   |                | \n",
    "                   |                |\n",
    "          [-0.5,0.5,0.5,-0.5]     [-0.5,0.5,0.5] \n",
    "                \"credit\"\n",
    "         <=50k(B)   |   >50k(G,N)\n",
    "            |----------------|\n",
    "            |                |\n",
    "            |                |\n",
    "            |                |\n",
    "         [-0.5]        [0.5,0.5,-0.5]\n",
    "        \n",
    "    now the proses will be same whether you want go to \"Bad\" side Spilt\n",
    "    or whethe i want to go for (\"good\",\"narmal\") side spilt so again\n",
    "    we calculate the \"Similarity weight\",and \"Informaion Gain\" until we\n",
    "    get the \"leafnode\"\n",
    "        \n",
    "        \n",
    "                       ∑(Residual)²\n",
    "\"similarity weight\" = --------------\n",
    "                       ∑P_r(1-P_r)+λ\n",
    "    \n",
    "      [:. λ is kinda \"hyper parameter\" ]   λ = 0 consider  \n",
    "        \n",
    "@similarity weight for <=50k(B):=   \n",
    "\n",
    "    ∑(Residual)² = [(-0.5)]² = 0.25\n",
    "    \n",
    "    \"P_r\" is comming from your \"base model\" that is 0.5\n",
    "    \n",
    "    ∑P_r(1-P_r)+λ = [0.5(1-0.5)+how many \"reiduals\" over here]\n",
    "    ∑P_r(1-P_r)+λ = [0.5(0.5)] = 0.25\n",
    "    \n",
    "   \"similarity weight\" for <=50k(B) = 0.25/0.25 = 1 comming 1 means\n",
    "its a \"leafnode\" here its \"purli splited\" [:. like \"Entropy\" = 1]\n",
    "  \n",
    "@similarity weight for (>50k)(G,N):=\n",
    "    \n",
    "    ∑(Residual)² = [0.5+0.5+(-0.5)]² = 0.25\n",
    "    \n",
    "    P_r is comming from your \"base model\" that is 0.5\n",
    "    \n",
    "    ∑P_r(1-P_r)+λ = [0.5(1-0.5)+how many no.of \"reiduals\"(error) over here]\n",
    "    \n",
    "    ∑P_r(1-P_r)+λ = [0.5(0.5)+0.5(0.5)+0.5(0.5)] = 0.75\n",
    "    \n",
    "    \n",
    "    \"similarity weight\" for (>50k)(G,N) = 0.25/0.75 = 0.333\n",
    "    \n",
    "    \n",
    "@similarity weight for \"root node\":=\n",
    "    \n",
    "    ∑(Residual)² = [-0.5+0.5+0.5+(-0.5)]² = 0\n",
    "    \n",
    "    \n",
    "    P_r is comming from your \"base model\" that is 0.5\n",
    "    \n",
    "    ∑P_r(1-P_r)+λ = [0.5(1-0.5)+how many reiduals over here]\n",
    "    \n",
    "    ∑P_r(1-P_r)+λ = [0.5(0.5)+0.5(0.5)+0.5(0.5)+0.5(0.5)] = 1\n",
    "    \n",
    "    \"similarity weight\" for \"root node\" is = 0/1 = 0\n",
    "    \n",
    "    \n",
    "                 salary SW = 0\n",
    "                      🔘   \n",
    "                      / \\   >50K\n",
    "               <=50K /   \\        \n",
    "                    /     \\\n",
    "           SW = 1 🔘      🔘  SW = 0.33\n",
    "            \n",
    "            \n",
    "    \n",
    "    [:.\"Child_1 node\" + \"Child_2 node\" - \"Rootnode\" = Information Gain]\n",
    "    \n",
    "    \"Information Gaine\" of \"credit\" = 1 + 0.3333 - 0 = 1.333\n",
    " \n",
    "\n",
    "Now  \"Information Gaine\" of \"credit\" > \"Information Gaine\"  Salry  \n",
    "                1.333 >   0.188\n",
    "    \n",
    "    like how spliting willhappen unless and untill \"leafnode\" is finded\n",
    "    \n",
    "like wise we will not stope here we will campare the same spilt also\n",
    "like we campare the \"credit\" spilt with like (Bad) and (Good,Notmal)or\n",
    "like (Bad,Good) and (Normal) spilt\n",
    "\n",
    "                          SW = 0.142\n",
    "              [-0.5,0.5,0.5,-0.5,0.5,0.5,-0.5]\n",
    "                         \"salary\"\n",
    "                 <=50K      |         >50k\n",
    "                   |--------------------|\n",
    "                   |                    | \n",
    "                   |                    | SW = 0.33\n",
    "          [-0.5,0.5,0.5,-0.5]       [-0.5,0.5,0.5] \n",
    "                \"credit\"SW=0                 |\n",
    "         <=50k(B)   |   >50k(G,N)        \"credit\"\n",
    "            |----------------|     <=50k(B,G)   |  >50k(N)   \n",
    "            |                |          |---------------|\n",
    "            |                |          |               |\n",
    "        SW=1|                |SW=0.33   |SW=0.33        | SW=1\n",
    "         [-0.5]        [0.5,0.5,-0.5] [-0.5,0.5,0.5]   [0.5] \n",
    "            \n",
    "                  this is \"Decision_Tree\" 1\n",
    "            \n",
    "                       ∑(Residual)²\n",
    "\"similarity weight\" = --------------\n",
    "                       ∑P_r(1-P_r)+λ \n",
    "    \n",
    "    \n",
    "    ∑P_r(1-P_r)+λ  =  this \"Denominater\" is our \"Cover value\"\n",
    "    \n",
    "    let say if my \"Cover value\"or \"Probability value\" is = 0.25\n",
    "    \n",
    "    if my Cover value < 0.25 if i Consider my \"Gain\" is\n",
    "         \n",
    "        \"Information Gain\" < 0.25 then cut my Tree that is called\n",
    "        as \"Post_pruning\"\n",
    "        \n",
    "        my Gain is 1.333 < 0.25\n",
    "        \n",
    "        as you go down your \"Cover value\" will be \"Decrese\" \n",
    "        \n",
    "I may Constact the any no.of \"Decision_Trees\" once i calculate the\n",
    "\"Resuals\"(Errors)\n",
    "    \n",
    "            \n",
    "            \n",
    "See here if you do spilt like\" Bad & Goog,Normal\" or \"Bad,Good & Normal\"\n",
    "the slpit will be same \n",
    "\n",
    "So finally we take which ever \"tree\" has good \"Information Gain\" that could\n",
    "be taken\n",
    "                \n",
    "        \n",
    "how calculation happen in Decision \"XG_boost\" tree     \n",
    "\n",
    "in \"Gradient_boost\"    \n",
    "h0(x) + α1 (DT1) + α2(DT2) + α1(DT3)+...+.αn(DTn)  \n",
    "  |-->basemodel  \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "Now in Xg_boost how calculation happens ?\n",
    "\n",
    "in \"base model\" (P_r)= 0.5\n",
    "\n",
    "\n",
    "\n",
    "    base model P_r=0.5          \n",
    "        |----| ------->🔘   \n",
    "        |----|         / \\   \n",
    "                      /   \\        \n",
    "                     /     \\\n",
    "                    🔘     🔘 \n",
    "        \n",
    "        \n",
    "now at this \"base model\" we apply \"log loss\" that is \n",
    "\n",
    "                       P_r\n",
    "     ->log(odds)=log -------\n",
    "                      1-P_r\n",
    "            \n",
    "    \n",
    "       = log(0.5/1-0.5)  = log(0.5/0.5) = 0  [:.if p_r = 0.5 ]   \n",
    "        \n",
    "So we apply \"log loss\" for the 1st \"base model\" to get the Output. \n",
    "\n",
    "         so here \"log loss\" Output is = 0 we go and see the record 0 o/p\n",
    "    so we go and see with respect 0 output record \"similarity weight\"\n",
    "    of the \"tree\" \n",
    "                              SW = 0.142\n",
    "              [-0.5,0.5,0.5,-0.5,0.5,0.5,-0.5]\n",
    "                         \"salary\"\n",
    "                 <=50K      |         >50k\n",
    "                   |--------------------|\n",
    "                   |                    | \n",
    "                   |                    | SW = 0.33\n",
    "          [-0.5,0.5,0.5,-0.5]       [-0.5,0.5,0.5] \n",
    "                \"credit\"SW=0                 |\n",
    "         <=50k(B)   |   >50k(G,N)        \"credit\"\n",
    "            |----------------|     <=50k(B,G)   |  >50k(N)   \n",
    "            |                |          |---------------|\n",
    "            |                |          |               |\n",
    "      \"SW\"=1|                |SW=0.33   |SW=0.33        | SW=1\n",
    "         [-0.5]        [0.5,0.5,-0.5] [-0.5,0.5,0.5]   [0.5] \n",
    "          |  \n",
    "          |  \n",
    "          |---> from here we get \"SW = similaity weight\"  = 1\n",
    "            \n",
    "    \n",
    "we will apply one \"activation function\".\n",
    "\n",
    " Xg_boost =  σ ((P_r of \"base madel\" with \"log loss\" aplyed value) + \n",
    "                         α1 (similarity weight DT1 ))\n",
    "\n",
    "                                  [:. α = it a \"hyper parameter\" ]\n",
    "    Xg_boost =  σ (0 + α1 (1))          [:. lets say α1 = 0.5]\n",
    "                                      \":. α alays ranging B/W 0 to 1\"\n",
    "    Xg_boost =  σ (0 + 0.5(1))= σ(0.5).\n",
    "    \n",
    "                                                     1\n",
    ":. σ =\"sigmoid activation function\" formula -> y = ------\n",
    "                                                    1+e-x\n",
    "                  1 \n",
    "    = σ (0.5) = -------- = suppose let say 0.6 this is my \"new probabity\"\n",
    "                 1+e-0.5    for the 1 record \n",
    "        \n",
    "        \n",
    "   \" Sigmoid activation function\" will always  b/w 0 to 1\n",
    "\n",
    "Why we want only 0 or 1 in the sence this is a classification problem.\n",
    "        \n",
    "        \n",
    "        similarly for next recocrd_2 i will apply this σ (0 + α1 (SW))\n",
    "        similarly for next recocrd_3 i will apply this σ (0 + α1 (SW))\n",
    "         ;\n",
    "         :\n",
    "        similarly for next recocrd_n i will apply this σ (0 + α1 (SW))\n",
    "        \n",
    "like we get the new probabily with respect to Records like say P_r like\n",
    "\n",
    "    salary          credit       Approval   Residal1  P_r  Residal2\n",
    "    \n",
    "     <=50K            B              0       -0.5     0.6  0-0.6=-0.6     \n",
    "     <=50K            G              1        0.5     0.6   1-0.5=0.5\n",
    "     <=50K            G              1        0.5     0.4       0.7\n",
    "     > 50K            B              0       -0.5     0.3      -0.3\n",
    "     > 50k            G              1        0.5     0.7      -0.8\n",
    "     > 50K            N              1        0.5     0.2       0.9\n",
    "     <=50K            N              0       -0.5     0.1      -0.9\n",
    "        \n",
    "        Now based on this \"P_r\" will get calculate the \"Resudual2\"\n",
    "        we calculate like \n",
    "          \n",
    "            0-0.6=-0.6 \n",
    "            1-0.5= 0.5\n",
    "            \n",
    "            now based on this new \"Resudul2\" again i calculate the\n",
    "            \"Decision_Tree\" till the End, so my formula will be like\n",
    "so finally            \n",
    "\n",
    "Xg_Boost = base model + σ { α1(DT1) + α2(DT2) + α1(DT3)+...+.αn(DTn) }\n",
    "\n",
    "  \n",
    "    once we apply this Sigmoid function My probabity will giv you good value\n",
    "    where my \"Residual\"(Error) will be very very less\n",
    "        \n",
    "\"sigmoid activation function\"  it ranges it always gives you\n",
    "value B/W [ 0 to 1 ] why we want 0 to 1 values only means \n",
    "becoze this is a \"classification\" problem        \n",
    "        \n",
    "    \n",
    "    σ = is bassically our 'sigmoid Activation' function\n",
    "    \n",
    "                                                     1\n",
    ":. σ =\"sigmoid activation function\" formula -> y = ------\n",
    "                                                    1+e-x\n",
    "    \n",
    "here in the place of \"base model\" we apply \"log loss\"\n",
    "and log \"base model\" \"P_r\" = 0.5\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366828d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3232fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fd3c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e012c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9d01048",
   "metadata": {},
   "source": [
    "# XGboost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec6f212",
   "metadata": {},
   "outputs": [],
   "source": [
    "@XGboost Regressor :=\n",
    "    \n",
    "Let say i have dataset like this:=\n",
    "    \n",
    "    \n",
    "    Exp       gap      salary   residual1\n",
    "    -------------------------  -----------\n",
    "    2         yes        40K      -11\n",
    "    2.5       yes        42k      -9\n",
    "    ---------------------------------\n",
    "    3          no        52K       1\n",
    "    4          no        60K       9   \n",
    "    4.5       yes        62K       11\n",
    "    \n",
    "    by seeing dataset that we know its Regression problem stmt becoz \n",
    "    \"output feature\" is having values are continous values.\n",
    "    \n",
    "       \n",
    "step :1 \n",
    "    \n",
    "in Xg_boost 1st we need to create \"Base Model\" ---> 51K\n",
    "\n",
    "\n",
    "   40+42+52+60+62\n",
    "  --------------- = 51\n",
    "       5\n",
    "\n",
    " \"Avarage\" of all the salarys = 51K\n",
    "    \n",
    "step:2:=\n",
    "    after \"base model\" we create \"Residual1\" column\n",
    "    \n",
    "    40-51=-11\n",
    "    42-51=-9\n",
    "    52-51=1\n",
    "    60-51=9\n",
    "    62-51=11 there are my Residuals\n",
    "    \n",
    "\n",
    "step3 := \n",
    "    with the help of \"exp\" \"gap\" and \"Residual\" we can create\n",
    "    create \"Decision_tree_1\" \n",
    "    \n",
    "    constract dession tree :=\n",
    "    \n",
    "         [-11,-9,1,9,11]\n",
    "               exp    \n",
    "                |  \n",
    "      <=2       |         >2        \n",
    "       |-------------------|       \n",
    "     [-11]            [-9,1,9,11]    \n",
    "   \n",
    "    \n",
    "step : 2 : =\n",
    "                       ∑(Residual)²\n",
    "\"similarity weight\" = --------------\n",
    "                       ∑P_r(1-P_r)+λ\n",
    "    \n",
    "    \n",
    "this formula change if i use \"Regression problem\"\n",
    "\n",
    "\n",
    "                         ∑(Residual)²\n",
    "\"similarity weight\" = ----------------------\n",
    "                       \"No.of residual\" + λ\n",
    "    \n",
    "    \n",
    "    \n",
    "@similarity weight for -11(<=2) :=\n",
    "    \n",
    "    \n",
    "    ∑(Residual)² = [-11]² = 121   \n",
    "       \n",
    "    No.of residual+λ = 1 + 1 = 2  [:.lets concider \"λ\" = 1]\n",
    "    \n",
    "                     [:. \"λ\" is  \"hyper parameter\" ]\n",
    "        \n",
    "    \"similarity weight\" for (-11)(<=2) = 121/2 = 60.5\n",
    "    \n",
    "@similarity weight for (-11)(>2) :=\n",
    "    \n",
    "∑(Residual)² = [-9+1+9+11]² = 144\n",
    "       \n",
    "    No.of residual + λ = 4 + 1 = 5   [:.λ=1]\n",
    "    \n",
    "        \n",
    "    \"similarity weight\" for -11(<=2) =144/5 = 28.8    \n",
    "    \n",
    "        \n",
    "@similarity weight for\" root node\" or base model :=        \n",
    "        \n",
    "    ∑(Residual)² = [-11+(-9)+1+9+11]² = 1\n",
    "       \n",
    "    No.of residual + λ = 5 + 1 = 6   [:.λ=1]\n",
    "    \n",
    "                    \n",
    "        \n",
    "    \"similarity weight\" for -11(<=2) =1/6 = 0.16666        \n",
    "   \n",
    "     \"similarity weight for root node\" = 0.16666\n",
    "    \n",
    "step :4 :\n",
    "        find out the \"Information gaine\"\n",
    "        \n",
    "    [:.\"Child_1 node SW\" + \"Child_2 node SW\" - \"Rootnode SW\" = Information Gain]\n",
    "\n",
    "    \"Information Gaine\" = 60.5 + 28.8 - 0.16666  = 89.133\n",
    "    \n",
    "Why this \"Information Gain\" will be used ?\n",
    "Ans:=\n",
    "    \"Information Gain\" will help you to select the \"features\"\n",
    "    suppose if we are not selcting from \"feature2\" then also\n",
    "    it will select the \"feature\" by calculating the \"Information Gain\"\n",
    "    \n",
    "  \n",
    "\n",
    "constract Dession_tree with (<=2.5) :=\n",
    "    \n",
    "         [-11,-9,1,9,11]\n",
    "               exp    \n",
    "                |  \n",
    "      (<=2.5)   |          >2.5        \n",
    "       |-------------------|       \n",
    "     [-9,-11]            [1,9,11]  \n",
    "                        \n",
    "\n",
    "\n",
    "\n",
    "@similarity weight for [-11,-9](<=2.5) :=\n",
    "    \n",
    "     \"Avarage\" of this is -11-9/2 = -10\n",
    "    \n",
    "∑(Residual)² = [-9-11]² = 400\n",
    "       \n",
    "    No.of residual+λ = 2 + 1 = 3   [:.λ = 1]\n",
    "    \n",
    "                    \n",
    "        \n",
    "    \"similarity weight\" for -11(<=2) = 400/3 = 133.3 \n",
    "\n",
    "@similarity weight for [1,9,11](<=2.5) :=\n",
    "    \n",
    "      \"Avarage\" of this is 1+9+11/3 = 7\n",
    "    \n",
    "∑(Residual)² = [1+9+11]² = 441\n",
    "       \n",
    "    No.of residual+λ = 3 + 1 = 4   [:.λ = 1]\n",
    "    \n",
    "        \n",
    "    \"similarity weight\" for -11(<=2) = 441/4 = 110.25 \n",
    "\n",
    "\n",
    "@similarity weight for \"root node\" or base model :=        \n",
    "        \n",
    "    ∑(Residual)² = [-11+(-9)+1+9+11]² = 1\n",
    "       \n",
    "    No.of residual+λ = 5 + 1 = 6   [:.λ=1]\n",
    "    \n",
    "                     [:. λ is kinda hyper parameter ]\n",
    "        \n",
    "    \"similarity weight\" for -11(<=2) =1/6 = 0.16666        \n",
    "   \n",
    "     \"similarity weight for root node\" = 0.16666\n",
    "    \n",
    "    \n",
    " [:.\"Child_1 node SW\" + \"Child_2 node SW\" - \"Rootnode SW\" = Information Gain]\n",
    "\n",
    "\"Information gaine\"\n",
    "\n",
    "   \"Gaine\" = 133.3 + 110.23 -0.166 =  243 \n",
    "    \n",
    "                 243 > 89.134\n",
    "\n",
    "which \"Decision\" tree gives you more \"Information gaine\"\n",
    "that can you use \n",
    "\n",
    " \n",
    "i selected the this Good \"Information Gain\" Dession_tree\n",
    "\n",
    "         [-11,-9,1,9,11]\n",
    "               exp    \n",
    "                |  \n",
    "      (<=2.5)   |          >2.5        \n",
    "       |-------------------|       \n",
    "     [-9,-11]            [1,9,11]\n",
    "    \n",
    "at root node the \"Avarage\" we need to calculate\n",
    " \n",
    "    at <=2.5                   at >2.5 \n",
    "    (-9)+(-11)                  (1+9+11)\n",
    "   ------------- = -10  ,       ---------- = 7   \n",
    "         2                          3\n",
    "    \n",
    "        |--->\"Base model\"  \n",
    "        |               \n",
    "    |--------|      |----------|    \n",
    "    | base   |--->  |   DT     |    \n",
    "    |  model |      |   2      |---> - - ......\n",
    "    |--------|      |----------|     \n",
    "                        /   \\               \n",
    "               (<=2.5) /     \\ (>2.5)  \n",
    "\"avarage\"=51         🔘      🔘     \n",
    "                     -10       7 \n",
    "        \n",
    "(<=2.5) and (>2.5) decision tree \"avarage\" is -10 , 7\n",
    "\n",
    "So suppose if i pass my 1st any new \"record\" or test \"record\".\n",
    "my base model is \"avarage\" of \"Ouput feature\".\n",
    "                                        [:. α = Learning rate]\n",
    "                                        [:.α = 0.5 concider  ]\n",
    "        \n",
    "    51 + α1(-10)  + α2(DT2)+.....+αn(DTn)\n",
    "\n",
    "    51 + 0.5(-10) = 51 - 5 = 46k \n",
    "\n",
    "the formula will become like\n",
    "\n",
    "Xg_Boost = \"Base model\" + α1(DT1) + α2(DT2) + α1(DT3)+...+.αn(DTn) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38744166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8682cb6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f68ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63073b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1867cae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eba3345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f17cc9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: list indices must be integers or slices, not tuple; perhaps you missed a comma?\n",
      "<>:1: SyntaxWarning: list indices must be integers or slices, not tuple; perhaps you missed a comma?\n",
      "C:\\conda_tmp/ipykernel_13912/1412630352.py:1: SyntaxWarning: list indices must be integers or slices, not tuple; perhaps you missed a comma?\n",
      "  [-9,-11]            [1,9,11]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\conda_tmp/ipykernel_13912/1412630352.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m]\u001b[0m            \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "[-9,-11]            [1,9,11] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d8ad4795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-20"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-9)+(-11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "58f52ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133.33333333333334"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "400/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "440b29e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110.25"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "441/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e5d8e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c75f0fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8254ea9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ba5336",
   "metadata": {},
   "outputs": [],
   "source": [
    "[-9,1,9,11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d8629388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "An \"Ensemble method\" is a \"technique\" that combines the predictions from \n",
    "multiple machine learning Alrgms together, to make more accurate predictions\n",
    "then any individual model. A model comprised of many models is called an \n",
    "\"Ensemble model\"\n",
    "\n",
    "here 2 types of Ensemble learning are there \n",
    "  \"homogenious Ensemble learning \"\n",
    "    here we combie use only same algorithem multyple times\n",
    "  \"heterogeneous ensemble learning\"\n",
    "    here we combine multiple different different Algorithems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5b402a",
   "metadata": {},
   "source": [
    "# Un_superwised Machine Leaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470efc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "What is Cluster ?\n",
    "\n",
    "Ans:= the \"Cluster\" is nothing but a its a \"Group\" of data points \n",
    "    that are \"similer\" to \"each other\" and dissimilar to datapoits\n",
    "    in other cluster/Groups \n",
    "    \n",
    "\n",
    "\n",
    "Un_superwised Machine Leaning :=\n",
    "    \n",
    "    \n",
    "    lets concider i am running  a mall in this mall every month\n",
    "    new products come\n",
    "             MAll <-------- New project \n",
    "  \n",
    "in this mall some peopele come so   \n",
    "\n",
    "some poeple \"eranig more\"      some poeple \"eranig less\"  \n",
    "            \"buying more\"                  \"buying more\"\n",
    "\n",
    "some poeple \"eranig more\"     some poeple \"eranig less\" \n",
    "            \"buying less\"                 \"buying less\"\n",
    "    \n",
    "    \n",
    "    so new product is come who poeple are going to buy \n",
    "    that product is ans := the \"above 2 people\" becoz they\n",
    "    they have high \"probabulity\" to buy that produact\n",
    "    \n",
    "    So here make 4 groups like \"4 Clusters\" (సన్నిహితమైన గుంపు) \n",
    "    i ll send mail to those people who like to buy products more with\n",
    "    15% they have high \"probability\" of bying the products\n",
    "    \n",
    "based on this i create 2 \"features\" like     \n",
    "\n",
    "salary    spending score\n",
    "100k           9\n",
    "50K            4\n",
    "120k           9\n",
    "40k            8\n",
    "70k            3\n",
    "\n",
    "see here we dont have any specific \"Optput feature\" i only have this\n",
    "\"2 information\" with the help of this \"2 information\" we can \"Group\" this\n",
    "people \"together\" that possible with the help of \"Un_superwised Machine Leaning\"\n",
    "Algorithems\n",
    "\n",
    "What is Un_superwised ML ?\n",
    "\n",
    "the mager aim of \"Un_superwised MACHINE LEARNING\" is to create \"Clusters\",\n",
    "\n",
    "what we can do with the help of \"Clusters\" ?\n",
    "\n",
    "    becoz  of this \"Clusters\" we can do we can take a \"specific action\" on\n",
    "    that on specific \"Clusters\", here there will be no \"Labled_Outputdata\",\n",
    "    there will be only \"features\" we will be focuss on making \"Cluster\" with\n",
    "    respect to \"Similar data \"\n",
    "    \n",
    " \n",
    "eX:= Netfliks\n",
    "    \n",
    "How many Models will be there Netfliks ?\n",
    "\n",
    " there are 1000s 1000s models will be there\n",
    "    \n",
    "    so here \"netflix\" creates \"Clusters\" Around the \"World\"\n",
    "    The \"cluster\" may be \"differen\" \"different\" with respect\n",
    "    different different \"regions\" with respect to different\n",
    "    different Languages,cultures, or somthing\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "2:22    \n",
    "{g}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\"Custume Ensimble Technic\" :=\n",
    "Ans:=\n",
    "    \n",
    "  2:30:1  \n",
    "    {g}\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "for soppose with respect different different \"Clusters\" we may Apply \n",
    "or try all the different types \"ML\" \"algorithems\" lets say i want to apply\n",
    "all the Classification and Regression alrgm like \"Linear\",\"logistic\",\"KNN\"\n",
    "like that out of this i select \"Best\" alrgm, \"similarly\" i do with all the \n",
    "\"Clusters\",b-coz each and every \"Cluster\" will be having differe \"dataset\",\n",
    "\n",
    "for the new test data like we said with respect to language we go to the\n",
    "some any cluster \n",
    "    \n",
    "for my \"Initill data\" we need to do \"Clustering\", from this \"Clustring\"\n",
    "i sent it to \"Independt models\",  this \"models\" is with respect to specific\n",
    "\"Clusters\", then probably i got the output, this is bassically called as\n",
    "\"Custume Ensimble Technic\".\n",
    "\n",
    "\n",
    "    \n",
    "lets say i 2 have feature      output is that  \n",
    " |             |               Discount to be sent in \"mail\"\n",
    "\"salary\"  \"spending score\"     \n",
    "100k           9               15%\n",
    "40K            8               12%  now this will become \n",
    "70k            4               5%   \"regression problem stmt\" b-coz the \n",
    "120k           8               15%   disscount will be in \"Continous data\"\n",
    "10k            1               13%  \n",
    "50k            4\n",
    "\n",
    "So here see based on the \"Clusters\" (Groups) the Discount will be sented \n",
    "\n",
    "let see the feature uppr side like  \"salary\" and \"spending score\"\n",
    "\n",
    "by seeing it we can create \"Similer Cluster\" with respect to Dataset definatly.\n",
    "\n",
    "let take \"salary\",\"spending score\" 2 \"features\" and create 3 \"clusters\"\n",
    "letsay \n",
    "\n",
    "After creating 3 \"Cluster\"  (datapoits) i can create/use \n",
    "different different \"Regression\" Algorithems,\n",
    "\n",
    "                    \n",
    "            at this \"cluster\" let say i got \"Rondom Forest regressor\"\n",
    "               is the best alrgm\n",
    "                        ◯\"Regression\"alrgms\n",
    "                      \n",
    "                    \n",
    "                    \n",
    "  here \"Ada_boost\"   \n",
    "     alrgm is best     \n",
    "      \"Regression\" ◯        ◯\"Regression\"alrgms\n",
    "        alrgms                  here \"XG_boost\" alrgm is best\n",
    "\n",
    "\n",
    "let say my when my \"new data\" will come it belongs any \"Cluster\"\n",
    " let say my data is belongs to \"Rondom Forest regressor\" Cluster\n",
    "So with the help of \"Rondom Forest regressor\" it will get predicted\n",
    "\"Output\" and based on the \"output\" it can sent to the same  \"Cluster\" only    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0fd5d7",
   "metadata": {},
   "source": [
    "# K Means Clustring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db4ba1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "K Means Clustring :=\n",
    "    \n",
    "     \n",
    "    aim :=\n",
    "        \n",
    "       x2\n",
    "       |\n",
    "       | \n",
    "       |  • • \n",
    "       |• •  •     \n",
    "       |• • •  * * *            \n",
    "       | • •  *  *  *            • ----> data_points 1 \n",
    "       |     * * *  *           * ----> data_points 2   \n",
    "       |      * *  *\n",
    "     --|---------------->x1   \n",
    "       |     \n",
    "    by seeing this we have 2 \"Clusters\"\n",
    "  \n",
    "Note:- this is not a classification problem just created 2 clusters that it\n",
    "    \n",
    "x1 ,x2 are my \"input features\" we are able to finded like 2(groups)\"Clusters\"      \n",
    "it can be 3 clusters also.\n",
    "\n",
    "but the thing is only in 1st instance how to find out this 2 \n",
    "\"Groups\"(Clusters) ?\n",
    "\n",
    "step:1:= \n",
    "        \n",
    "               x2\n",
    "       |\n",
    "       | \n",
    "       |🔸 • • \n",
    "       | • •  •     \n",
    "       | • • •     * * *            \n",
    "       |  • •     *  *  *          * &  • ----> same data_points  \n",
    "       |        * * *  *           \n",
    "       |         * * *  🔹        🔹& 🔸-----> \"Centroids\"\n",
    "     --|---------------->x1   \n",
    "       |     \n",
    "        \n",
    "for creating \"Clusters\" we creates 2 independent \"Centroides\"\n",
    "or \"K Centroids\", it can be initialize any where.  after creating\n",
    "this \"Centroids\" we will try to \"calculate\"  distance of \"clusters\"\n",
    "points and \"Centroid\" points. \n",
    "\n",
    "    [Note := \"Centroids\" can be \"initilized\" any whare .]\n",
    "\n",
    "which ever \"point\" are near to \"Centriod points\" will belong to \n",
    "that specific \"Cluster\" \n",
    "\n",
    "How to find out distance b/w clusters points and Centroids ?\n",
    "Ans:-\n",
    "    we use \"Euclidean\" or \"Manhatten\",\"minkowiski\" to find out the\n",
    "    distance b/w them.\n",
    "     \n",
    "One more way simpuler way is \n",
    "\n",
    "i create one line with respect to \"2 Ctroides\" and one 90 degree\n",
    "line like one \"perpendiculer\" lines at middle and these line shows\n",
    "all these poists that are near to the \"specific points\" and create\n",
    "saperate of it.\n",
    "But internully doing \"Euclidian Distance\" calculation\n",
    "\n",
    "       |\n",
    "       |         /    \n",
    "       |🔸•••   / \n",
    "       | • \\ • / *      \n",
    "       | • •\\ /*  ** *            \n",
    "       | •• /\\* * * *             * &  • ----> same data_points  \n",
    "       |   /  \\ * * * *           \n",
    "       |  /   🔹 * * *          🔹& 🔸-----> \"Centroids\"\n",
    "     --|---------------->x1   \n",
    "       |     \n",
    "        \n",
    "now one side all points \"Avarage \" will get calculated and then\n",
    "one side of \"Centroid\" will now get moved towords \"new Centroide\" \n",
    "new \"Centroid\" will be the \"avarage\" of all points.\n",
    "\n",
    "       |\n",
    "       |        /    \n",
    "       |🔸 ••  / \n",
    "       | • \\ •/*                 ♦ ----> \"avarage\" of all points (•)\n",
    "       |• ♦ \\/*  ** *             \n",
    "       | •• /\\* * * *             * &  • ----> same data_points  \n",
    "       |   /  \\ * ✿ * * *           \n",
    "       |  /   🔹* * *           🔹& 🔸-----> \"Centroids\"\n",
    "     --|---------------->x1     ✿ ---->\"avarage\" of all points (✿)\n",
    "    \n",
    "for the \"New Centroids\" will get calculated also again repeated same.\n",
    "\n",
    "and next we repeat the same process what we did before where, which ever \n",
    "points are nearer to the \"Centroid\", and again we create paralel line \n",
    "with respect to \"Centroids\" again my Centroids \"Avarage\" will get calculated.\n",
    "when it will get fixed to the particuler Groups then it will get Created\n",
    "like \"Clusters\"\n",
    "    \n",
    "    \n",
    "but why not \"Centroids\" \"K\" = 2 or K=3,4,5... ?\n",
    "\n",
    "if k = 1  k= 1 to 20\n",
    "                               \n",
    "                                if i take k = 1 with in \"cluster\" \n",
    "                                \"some of square\" will be very \"high\"\n",
    "            \n",
    "                                if i take K= 2 \"centroids\"  with in\n",
    "2:46:10                         \"cluster\" \"some of square\" will be\n",
    "{g}                             \"decreas\", when compare to k = 1\n",
    "\n",
    "                                similarly if i take K=3,4,5..with in\n",
    "2:46:10                         \"cluster\" \"some of square\" will be\n",
    "{g}                             keep on \"decreasing\" \n",
    "\n",
    "\n",
    "Now you have to create a \"Graph\" which will bassically\n",
    "having 2 things \"K\" and \"WCSS\"\n",
    "\n",
    "with in\n",
    "\"Cluster Some of Square\"                             \n",
    "     ^ \n",
    "     |\n",
    "     | *\n",
    "     | *\n",
    "     | * \n",
    "     |  *                   my graph will \"Decrese\" like this\n",
    "     |    *                  if i take K = 1,2,3,4...\n",
    "     |      *   *   *  \n",
    "     |--------------------> \"K\" \n",
    "    \n",
    "How do we find out \"K\" or \"Centroids\" ?\n",
    "                                             [:. \"K\" = No.of Centroids]\n",
    "\n",
    "we have \"L_Bow method\", where ever you find out \"upbropt change\"\n",
    "that point should be taken as a  No.of \"centroids\" , \"upbropt change\"\n",
    "means saden down change and it becomes stable \n",
    "\n",
    "one more challenge you face with respect to \"K-means Clustering\"\n",
    "\n",
    "let say i have datapoints like this ?\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "if sopposese we initialize 3 \"Centroids\" very near to each other datapoints\n",
    "then there is a possibility that \"Wrong Clusters\" may also be formed so we\n",
    "need to fix this  Wrong \"Clusters\",\n",
    "\n",
    "in \"K means\" there will there will be a \"parameter\" which is called as\n",
    "\"K means ++\"\n",
    "\n",
    "what is the use of \"K means ++\" ?\n",
    "Ans:=\n",
    "    \"K means ++\" will be \"parameter\" that is  used in \"K means Clustring\"\n",
    "    and this \"K means ++\" always make sure that your \"Centroids\" are very\n",
    "    for or away initialized from the each other. \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff4c403",
   "metadata": {},
   "source": [
    "# Heirarichal mean Clustaring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6de5bde5",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (conda_tmp/ipykernel_13912/1715650830.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\conda_tmp/ipykernel_13912/1715650830.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Heirarichal mean Clustaring :=\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Heirarichal mean Clustaring :=\n",
    "    this \"Heirarichal mean Clustaring\" is a \"Unsuperwised\" Machine Learning\n",
    "    Algorithem.\n",
    "    in \"Heirarichal mean Clustaring\" we have to \"Group Clusters\"\n",
    "    based on \"Dendrogram\".its kinda of \"Bottum\" to \"Top\" approuch\n",
    "    \n",
    "    *the other Name of \"Heirarichal mean Clustaring\" is \"AgglomerativeClustering\"\n",
    "    \n",
    "    \"affinity\" = affinity s nothing but \"euclidean\" distance or \"manhatten\" \n",
    "     distance it calculates\n",
    "                                     \n",
    "                                     ^\n",
    "                                     |\n",
    "let say these are my points        --|--      |------------| \n",
    "     ^                               |        |            |\n",
    "     |            *p1              --|--  |-------|        |\n",
    "     |               *p2             |    |       |        | = \"Dendogram\"\n",
    "     |                             __|____♦_______♦________♦____ \n",
    "     |                               |    |       |        |  so here the line\n",
    "     |                             --|--  |     |---|      | passes 3 region  \n",
    "     |   *p6                         |    |     |   |      |   so Clustars = 3\n",
    "     |               *p3           --|--  |     |   |    |---|\n",
    "     |   *p5                       --|- |---|   |   |    |   |\n",
    "     |            *p4                |  |   |   |   |    |   |\n",
    "     |-------------------->          |--|---|---|---|----|---|-->\n",
    "                                         p1  p2  p3 p4    p5  p6\n",
    "    \n",
    "in \"Heirarichal mean Clustaring\" on the 1st \"instance\" we will not create\n",
    "\"clusters\", in this \"Heirarichal mean Clustaring\" we take each and every\n",
    "point \"initially\" as a \"Clustar\", \n",
    "\n",
    "see here \"p1\" and \"p2\" points are near so that i combine these 2 points.\n",
    "together. and based on this i will \"create\" a graph like bar chart.\n",
    "now then \"p5\" and \"p6\" are near points so that we combine them together.\n",
    "if i combine this \"p5\" and \"p6\" this \"Euclidean Distance\" much more then \n",
    "the \"p1\" & \"p2\" similarly \"p3\" and \"p4\"\n",
    "\n",
    "after \"Combining\" points we will look for which \"Clusters\" are near to \"each other\".\n",
    "those \"Clustars\" will be \"Combined\" together, and similarly the next where ever\n",
    "\"Clustars\" will be there that will be Combined together \n",
    "\n",
    "Now the quetion will be How do you find out How many \"Clustars\" are there\n",
    "in a graph ?\n",
    "\n",
    "Ans: just by seeing\n",
    "\n",
    "    \n",
    "what we do with the help od \"Dendrogram\" ?\n",
    "Ans:=\n",
    "with the help \"Dendrogram\" we will find how many \"Cluster\" we are having,\n",
    "to find out the 'clusters' in \"Dendrogram\" try to cut the \"longest verticle line\"\n",
    "such that nun of the \"horizental line\" passes through it.\n",
    "\n",
    "and create big \"horizental line\" and see \"how may regian\" that is passing\n",
    "through  thats the answer of Clustars how many Clustars we are having\n",
    "\n",
    "\n",
    "what is Dendrogram ?\n",
    "\n",
    "ans:=\n",
    "    Dendrogram bassically means we will try to find out the distance b/w \n",
    "    point and point and it just calculate the distance b/w the points.\n",
    "    and after grouping it together you basically find out the \"Dendrogram\"\n",
    "    it bassically means a path like A to B and do \"With in Cluster sum of\"\n",
    "    \"Square\". and then  combine , so overall path will start combining, \n",
    "    \n",
    "    \n",
    "    \"Dendrogram\" is a longer proces when Compare to\" K-means\" b-coz it try \n",
    "    to \"Combine\" every \"point to point\" so its a longer proces.so it \n",
    "    also take more time.\n",
    "\n",
    "    Note :=\n",
    "        if you want to yous \"Heirarichal Clustaring\" you have to use\n",
    "        it for \"smaller\" dataset \n",
    "        \n",
    "        if you want to yous \"K-Means Clustaring\" you have to use\n",
    "        it for \"larger\" dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ec52bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
