{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5efacf9",
   "metadata": {},
   "source": [
    "# Ada Boost Algrm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22da4af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "{IQ}what is the difference B/W wight box model and black\n",
    "box model ?\n",
    "\n",
    "Ans :=2:35:22\n",
    "    when we combine allthe \"enseble technique\" those are actually called as \n",
    "    blax box model\n",
    "  \n",
    "{IQ} what is data leakage ?\n",
    "\n",
    "Ans := as soon as i get my dataset i do \"Train_data\" and \"Test_data\"\n",
    "    split , so here for only \"Train_data\"i will be giving to my \"MODEL\"\n",
    "    for \"Training\" perpose so what exactly the \"leackage\" in the sense \n",
    "    my \"Model\" does not have any idia about my \"Test_data\",if it has any\n",
    "    idia about my \"Test_data\" then it will lead to a \"Dataleakage\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f0bac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac231761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6ced72b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14285714285714285"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f89052e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8556c58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "1:59:00    \n",
    "    \n",
    "      alrgm-1         alrgm-2        alrgm-3     alrgm-4      alrgm-5\n",
    "    |--------|      |----------|    |--------|   |-------|    |------|\n",
    "    | physics|--->  |geography |    |        |   |       |    |      |\n",
    "    |  guy   |      | guy      |--->|        |-->|       |--->|      |\n",
    "    |--------|      |----------|    |--------|   |-------|    |------|\n",
    "       |                 |                                         |\n",
    "       |                 |                                         |\n",
    "   \"weak learner\"     \"weak learner\"                    \"strong learner\"\n",
    "\n",
    "\n",
    "    this is \"boosting techniqe\" we will be combinning \"weak learners\"\n",
    "    \"together\" to make a overall \"strong learner\"  over here you have\n",
    "    \"Sequential\" model,\n",
    "    \n",
    "    \n",
    "    \n",
    "How this \"Boosting\" techniques get applyed in Ada boost ?\n",
    "\n",
    "    \n",
    "\"Ada Boost\" :=\n",
    "    \n",
    "    Aim := Ada Boost is a \"Boosting technique\" now in this Ada boost\n",
    "        what happens is that \"Wrong prediction\"(\"week learner\") will\n",
    "        be become an input for the Next \"Decission_Tree\"(\"stump\") so\n",
    "        bcoz of that we need to come up with \"higher\" accuracy\n",
    "        \n",
    "    \n",
    "lets i have some \"features\"  with 7 rocords\n",
    "\n",
    "\n",
    "f1      f1     f3       output  weights      updated\n",
    "                                             weight   \n",
    "--      --     ---       yes      1/7       0.058 %0.697\n",
    "---     --     ---       no       1/7       0.058 %0.697 \n",
    "---     --     ---                1/7       0.058 %0.697\n",
    "\n",
    "---     --     ---                1/7 ->    0.349 %0.697 lets say this is wrong\n",
    "\n",
    "---     --     ---                1/7       0.058 %0.697\n",
    "---     --     ---                1/7       0.058 %0.697\n",
    "---     --     ---                1/7       0.058 %0.697\n",
    "                                ----------------- ---\n",
    "                            sum    1        0.697\n",
    "        \n",
    "       formula  W = 1 / n [:. n= no.of records ] \n",
    "    \n",
    "        \n",
    "in \"Ada boost\" what we do i that we assign \"Weights\" \n",
    "\n",
    "How this is \"Weights\" are assigned.?\n",
    "\n",
    "how many \"records\" we are having. here \"7\"  so every \"recored\" will have\n",
    "a equal \"weight\". so innitially 1/7 for all the records.\n",
    "\n",
    "\n",
    "\n",
    "in \"Ada Boost\" we will take up all the \"Train_data\" records.we will train\n",
    "from 1st \"Decission_Tree\",in Ada_Boost also we use \"Decission_Trees\",\n",
    "now in \"Decission_tree\" it will get created with the help of one \"Depth\"\n",
    "\n",
    "\n",
    "                      ðŸ”˜ \"Decission_tree\"   \n",
    "                      / \\    \n",
    "                     /   \\       only 1 level \"depth\" \n",
    "                    /     \\\n",
    "                  ðŸ”˜      ðŸ”˜\n",
    "            \n",
    "        \n",
    "Now this \"one level depth\" this is specifically called as \"Stumps\"\n",
    "scence we really need to constract \"week learner\" so the \"Stumps\"\n",
    "which is having only one \"depth\"\n",
    "\n",
    "\"Stump\" = \n",
    "   \"Decission_Tree\" with 1 level and it should be a \"binari tree\"\n",
    "    means 2 binary output should have.\n",
    "        \n",
    "how does a \"Decission_Tree\" get selected ?\n",
    "    \n",
    "i can use  f1 and i can create a \"stump\" or\n",
    "i can use  f2 and i can create a \"stump\" or\n",
    "i can use  f3 and i can create a \"stump\"\n",
    "    \n",
    "\n",
    "      f1                  f2               f3\n",
    "      ðŸ”˜                 ðŸ”˜              ðŸ”˜      \n",
    "      / \\                / \\              / \\      \n",
    "     /   \\              /   \\            /   \\        \n",
    "    /     \\            /     \\          /     \\\n",
    "  ðŸ”˜      ðŸ”˜        ðŸ”˜      ðŸ”˜      ðŸ”˜      ðŸ”˜\n",
    "        \n",
    "initially all the weights having same weights\n",
    "we will take all features and we will train from \n",
    "1st decision tree\n",
    "        \n",
    "out of these which \"Decission tree\" be get selected ?\n",
    "Ans:=\n",
    "    with the help of \"Entropy\" or \"Gini Index\" and  \"Iformation Gain\" \n",
    "    the \"Decission_Tree\" will get Selected,when i combine all the things\n",
    "    like  \"Entropy\" or \"Gini Index\" and  \"Iformation Gain\" i can come up with\n",
    "    which  feature \"Stump\" has the \"spilt\" is good,\n",
    "\n",
    "    \n",
    "Let say if \"specific record\" has been given to the \"Decission_Tree\"(Stump)\n",
    "it will give some wrong Output, remaining all are fine.\n",
    "note := you can also use 1 \"record\" 2 \"records\" as your wish\n",
    "    \n",
    "    \n",
    "Now what will happen when 1 record bassically getting wrong ?\n",
    " Ans:=\n",
    "    if 1 record \"getting\" wrong now what exactly happen with this\n",
    "    \"Week learner\".\n",
    "    \n",
    "    1.We calculate \"Total Error\" \n",
    "    2.after calculate \"Total Error\" we calculate \"Performance of Stump\"\n",
    "        \n",
    "\n",
    "step:1\n",
    "\n",
    "\"Total Error\" :=\n",
    "    \n",
    "    \"Total Error\" = how many wrong records are there we need to concider \n",
    "    \"Total Error\" = 1/7\n",
    "    \n",
    "Step :2 \n",
    "    \n",
    "\"Performance of Stump\" :=\n",
    "    \n",
    "       1       1-TE    [:.TE = \"True Error\"]\n",
    "    = ---loge(------)\n",
    "       2         TE\n",
    "        \n",
    "       1      1-1/7\n",
    "    = ---loge(------) = 0.895 i am getting better, but the more the \"Error\"(TE)\n",
    "       2        1/7           the bad the \"Performance of Stump\" will be\n",
    "            \n",
    "   Note :-\n",
    "           \"Performance of Stump\" value more towords 1 means we getting \n",
    "           better \"performance\" of the \"Stump\".\n",
    "            \n",
    "        \n",
    "why we are calculating this \"Total Error\" and \"Performance stump\" ?\n",
    "Ans:=\n",
    "    reson we are calculating this \"Total Error\" and \"Performance stump\"\n",
    "    B-coz after creating one \"Stupm\"(Decission_tree) We need to update\n",
    "    the \"weight\",\n",
    "    \n",
    "    \"updating\" the \"weight\" will help us to deside that which record i\n",
    "    can send to it to the next \"decision_tree\" or \"weak learner\" \n",
    "    \n",
    "    in \"Boostin technic\" what happens is that, only the \"wrong record\" \n",
    "    from this particuler \"Decission_tree\"(stump 1) will be pass to the\n",
    "    next \"Decission_tree\" or the next \"stump\"\n",
    "    \n",
    "    \n",
    "    we should \"increas the Weights\" of the \"wrong prediction\" we \n",
    "    should \"decreas the Weight\" of the \"right prediction\".\n",
    "    \n",
    "why we need to \"increse\" the \"Wrong Preiction\" ?\n",
    "\n",
    ":= b-coz i need to send this \"Wrong Preiction data\" to my next\n",
    "    \"Decission_Tree\" \n",
    "\n",
    "How to \"increse Weights\" for \"wrong prediction\" & how to \"decrese Weights\" \n",
    "the \"currect prediction\" ?  \n",
    "\n",
    "for this we calculate \"New sample weight\" this will be for the\n",
    "\"currect Records\" \n",
    "    \n",
    "for \"Carrect records\" to \"decrese\" formula is :-\n",
    "    \n",
    "    \"New sample weight\" = weight *e-ps  [:. ps = \"performence of stump\"]\n",
    "        \n",
    "                        = (1/7) * e-0.895\n",
    "                        = 0.058\n",
    "                \n",
    "Note:-\n",
    "    \"wrong prediction\" should become an input for the next \"Decission_tree\"\n",
    "                \n",
    "---------------------------------------------------------------                \n",
    "f1      f1     f3       output  weights      updated\n",
    "                                             weight   \n",
    "--      --     ---       yes      1/7       0.058 /0.697\n",
    "---     --     ---       no       1/7       0.058 /0.697 \n",
    "---     --     ---                1/7       0.058 /0.697\n",
    "\n",
    "---     --     ---                1/7 ->    0.349 /0.697 lets say this is wrong\n",
    "\n",
    "---     --     ---                1/7       0.058 /0.697\n",
    "---     --     ---                1/7       0.058 /0.697\n",
    "---     --     ---                1/7       0.058 /0.697\n",
    "                                ----------------- -----  we have t do sum of all \n",
    "                            sum    1        0.697        such it becomes towards 1\n",
    "------------------------------------------------------------------\n",
    "\n",
    "\n",
    "for \"Incurrect records\"  to \"increse weight\" formula\n",
    "\n",
    "          \"New sample weight\" = weight *e+ps   [:. ps = \"performence of stump\"]\n",
    "                              = 1/7 * e+0.897\n",
    "                              = 0.349\n",
    "            \n",
    "            \n",
    "\"Normalized weight\"(decrese) for all the sumation of \"weight\" will\n",
    "be equals to 1\n",
    "\n",
    "0.083 decreas the weight\n",
    "0.083\n",
    "0.083\n",
    "\n",
    "0.500 increase the weight\n",
    "\n",
    "0.083\n",
    "0.083\n",
    "0.083\n",
    "-------\n",
    "   1\n",
    "    \n",
    "Why we are \"incresing weight\" and why we are \"decresing weight\"?\n",
    "Ans:=\n",
    "    the reson why we need to \"increase the weight\" and \n",
    "    \"decresing weight\" is that b-coz i should probably in the next\n",
    "    \"Decission_Tree\" that we are acually creating ,there should be\n",
    "    high \"probability\",that we should be able to give the \"weekLearner\"\n",
    "    \"wrong predicted record \" means giving priority for the \n",
    "    \"wrong predicted records\"\n",
    "\n",
    "Ater that we will do that \"backeting\".\n",
    "\n",
    "\"Bucketing\" :=\n",
    "    here we just create buckets when we create a bucket\n",
    "    the \"maxmum bucket size will be wrong records\" only. \n",
    "    1st bucket--> [ 0     to 0.083  ]  \n",
    "                  [ 0.083 to 0.166  ]  0.083 + 0.083 = 0.166\n",
    "                  [ 0.166 to 0.249  ]   0.166 + 0.083 = 0.249\n",
    "                  [ 0.249 to 0.7497 ]   0.249 + 0.500 = 0.7497 ^\n",
    "                  [                 ]  \n",
    "                    \n",
    "if the next Decission_tree if it get constracted  if i generate\n",
    "a \"random number\"  B/W [0-1] so the maximum no.of times \"wrong predicted\"\n",
    "data will only get selected for the Next \"Decission_tree\", lemaon way\n",
    "maximum no.of \"wrong predicted\" results will go to the next \"Decision_Tree\".\n",
    "\n",
    "\n",
    "if there are 100`s of records the probability of select \"wrong predicted\"\n",
    "will have high chance.for the Next \"Decission_tree\"\n",
    "\n",
    "Note:-\n",
    "    b-coz of the Bucckect size is huze there is higher probability of\n",
    "    selecting \"wrong predicted\"\n",
    "    yes other records also get selected for the \"Training perpose\" but\n",
    "    maximum no.of time \"wrong predicted\" will get selecte for next \n",
    "    Decission_tree\n",
    "\n",
    "what will happen at the end of the day  :=\n",
    "    \n",
    "     combine all \"together\" \"sequentially\" then it become \"Strong one\"\n",
    "         |----------------------------|--------------------------|\n",
    "         |                |             |                        | \n",
    "    |--------|      |----------|    |--------|                |------|\n",
    "    | DT     |--->  |   DT     |    | DT     |                | DT   |\n",
    "    |  1     |      |   2      |--->|   3    |- - - - - - - ->|  100 |\n",
    "    |--------|      |----------|    |--------|                |------|\n",
    "      / \\                / \\            / \\                     / \\   \n",
    "     /   \\              /   \\          /   \\                   /   \\        \n",
    "    /     \\            /     \\        /     \\                 /     \\\n",
    "  ðŸ”˜      ðŸ”˜        ðŸ”˜      ðŸ”˜    ðŸ”˜      ðŸ”˜             ðŸ”˜      ðŸ”˜\n",
    "     \"WL\"               \"WL\"            \"WL\"                    \"WL\"\n",
    "        \n",
    "here each and every \"Decission_Tree\" is a \"week learner\" but when we \n",
    "combine all \"sequentially\" then it become \"stronge one\"\n",
    "\n",
    "\n",
    "For the new \"Test_data\" how it will give the Output ?\n",
    "Ans:-\n",
    "    let say if its a \"classification\" broblem,\n",
    "    every Decission_tree will give different different outputs\n",
    "    with the help of  \"majority of ovting\" we will get the answer\n",
    "\n",
    "\n",
    "    in the case of \"Regresion\" we need to take \"avarege\" of all\n",
    "    \"predictions\"/Outputs and that will be our \"mean\",\n",
    "\n",
    "    \n",
    " How many records will get selected every time?\n",
    "\n",
    "Ans:-\n",
    "    based on how many time we are itterate tgrough it.like\n",
    "    it will just itterate the no.of records that are present.\n",
    "\n",
    "here if the \"Entropy\" is less for f1 \"stump\" we gonna select that \n",
    "\"Decission tree\" as my base leaning model \n",
    "\n",
    "SAMMER.R := the output your get in the \"probabilities\"\n",
    "    Oput is \"probabilities\" bassically means\n",
    "    Ans:=\n",
    "        lets my 1st Decission_Tree 0.982 \n",
    "                2nd Decission_Tree 0.32 \n",
    "                3rd Decission_Tree 0.75 \n",
    "                4th Decission_Tree 0.64 if calculate Sumation if i get \n",
    "                            > 0.5 i can write it as 1\n",
    "                    \n",
    "            after this we canalso use \"roc_auc_curv\" to define this\n",
    "            threshold(0.5)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "for suppose my decission tree classifies 1 incurrect and 4 classifies\n",
    "currectly for this incurrect classification we have to  find out the \n",
    "\"Total Error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a46d4986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7746"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.7663+0.0083  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c97bf64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14285714285714285"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.14285714285714285*1**0.895"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5094e349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999999"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1/6)+(1/6)+(1/6)+(1/6)+(1/6)+(1/6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09c83db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999998"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1/7)+(1/7)+(1/7)+(1/7)+(1/7)+(1/7)+(1/7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1715d068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.998"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.083*6)+0.500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99370acf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
