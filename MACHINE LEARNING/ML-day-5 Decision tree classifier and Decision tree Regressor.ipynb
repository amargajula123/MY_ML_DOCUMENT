{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1605b5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "Agenda :=\n",
    "    Decision tree \"classifier\" and Decision tree \"Regressor\"\n",
    "    \n",
    "{IQ}\n",
    "\n",
    "what is the aprch to fill if the \"nan values\" incase of \"output feature\" ?\n",
    "Ans:=\n",
    "    convert this \"Nan\" values data into to \"testdata\"\n",
    "    make a combination of 70% 30% always make sure that \"NAN\"\n",
    "    values goes to the \"tet_data\" and train the data with\n",
    "    70% of the data and predict for this 30%(testdata)with\"nan\" values\n",
    "    \n",
    "    take out this entire \"records\" which has \"Nan\" values and\n",
    "    letter on combine with the \"test data\" thats it\n",
    "    and remaining all the Train_data and do it train and \n",
    "    predict the testdata\n",
    " \n",
    "Which is faster \"Gini Index\" or \"Entropy\"?\n",
    "Ans:=\n",
    "    \"Gini Index\" is faster it range B/W [0 to 0.5]\n",
    "     \n",
    "    why Gini Index faster then Entropy ?\n",
    "    \n",
    "    in \"Entropy\" it the \"log\" is used and it ranges B/w [0 to 1]\n",
    "    \n",
    "for a bigger dataset what will be best suitable ?\n",
    "Ans :=\n",
    "    \"Gini Index\"\n",
    "    \n",
    "which will give good Accuracy \"Entropy\" or \"Gini Index\" ?\n",
    "\n",
    "Ans :=\n",
    "    both will give good \"Accuracy\" but \"Entropy\" will going to take\n",
    "    more \"Time\".\n",
    "    \n",
    "    \"Infomation Gail\" will happen for the \"entire Tree\"\n",
    "    \n",
    "    \"Entropy\" and \"Gini Index\" will happen for the node itself\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7957b55b",
   "metadata": {},
   "source": [
    "# Decision Tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53b64edb",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 44)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m44\u001b[0m\n\u001b[1;33m    2-yes/3-no             |           3-yes/2-no\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "\"Decision_Tree\" classifier and \"Decision_Tree\" Regressor :=\n",
    "    \n",
    "what all things \"Decision_Tree\" helps us to solve  ?\n",
    "\n",
    "1. \"Regression\" Problem\n",
    "2. \"Classification\" Problem\n",
    "\n",
    "what is Decision_Tree ?\n",
    "Ans =\n",
    " i can say \"Decision_Tree\" has nested if else class\n",
    "\n",
    "Ans:=\n",
    "    \"Decision_Tree\" has nestead if else class.\n",
    "    \n",
    "    if (prsn_ag<21):\n",
    "        print('the person should be collage')\n",
    "    else  (prsn_ag>=21 and prsn_ag<=31): \n",
    "        print('the person is working')\n",
    "    elif(prsn_ag>30 and prsn_ag<=35):\n",
    "        print('he owns a bussiness')\n",
    "    else  :\n",
    "        print('he retired')\n",
    "        \n",
    "                 (prsn_ag<21)\n",
    "                     🔘\n",
    "               Yes  /  \\  No\n",
    "                   /    \\\n",
    "                  /      \\\n",
    "       (collage)🔘       🔘(prsn_ag>=21 and prsn_ag<=31)\n",
    "                          /\\\n",
    "                     Yes /  \\ No\n",
    "                        /    \\\n",
    "             (working)🔘     🔘(prsn_ag>30 and prsn_ag<=35)\n",
    "                             / \\\n",
    "                       Yes  /   \\ No\n",
    "                           /     \\\n",
    "              (bussiness)🔘      🔘'he retired'\n",
    "                        \n",
    "                        \n",
    "    So internally its if else forming like \"Decision_Tree\"\n",
    "    \n",
    "So similarly we use \"Decision_Tree\" to sovle \"Conditinal\" Problem.\n",
    "        \n",
    "        \n",
    "        \n",
    "    in Decision tree there are 2 types of \"technics\" are there \n",
    "    1. ID3 algorithem  2. CART (sklearn)\n",
    "    \n",
    "    \"CART\" basically means \"Classification\" and \"Regression\"\n",
    "    \"Tree Algorithem\" in CART we bassically create a binary\n",
    "    tree\n",
    "    \n",
    "----------------------------------------------------- |\n",
    "day   |  outlook   |  temperature | humadity | wind   |play_tennies \n",
    "----------------------------------------------------- |--------------\n",
    "d1       sunny          hot          high      weak        no\n",
    "d2       sunny          hot          high      strong      no\n",
    "d3       overcast       hot          high      weak        yes\n",
    "d4       rain           mild         high      weak        yes\n",
    "d5       rain           cool         normal    weak        yes \n",
    "d6       rain           cool         normal    strong      no\n",
    "d7       overcast       cool         normal    strong      yes\n",
    "d8       sunny          mild         high      weak        no\n",
    "d9       sunny          cool         normal    weak        yes\n",
    "d10      rain           mild         normal    weak        yes\n",
    "d11      sunny          mild         normal    strong      yes\n",
    "d12      overcast       mild         high      strong      yes\n",
    "d13      overcast       hot          normal    weak        yes \n",
    "d14      rain           mild         high      strong      no  \n",
    "\n",
    "Dataset is about based on \"input features\" wether the person is \n",
    "going to \"play_tennies\" or not\n",
    "\n",
    "when we do coding with \"Sklearn\" we specifically use \"CART\" tequniqe.\n",
    "b-coz its more efficient when compar to \"ID3\"\n",
    "\n",
    "How \"Decision_Tree\" Actually Work ?\n",
    "\n",
    "Note :=\n",
    "    \"ID3\" and \"CART\" almost have same functionality but \"CART\"\n",
    "    always leeds \"binary classification\", where us \"ID3\" will not\n",
    "    lead to \"binary classification\",\n",
    "    \n",
    "let say we want to create \"Decision_tree\" for the above dataset\n",
    "lets pick up any one \"feature\" \n",
    "Let say \"Outlook\" feature.\n",
    "\n",
    "in \"Outlook\" 3 \"unique categories\" are there\n",
    "\n",
    "                    9 Yes/5 No`s with respect \"outlook\"\n",
    "                         🔘\"outlook\"\n",
    "                         /|\\\n",
    "                        / | \\     So here see \"outlook\" has \"3 categories\" \n",
    "                       /  |  \\    (sunny,overcast,rain) so these could be\n",
    "                      /   |   \\    a \"ID3\" problem stmt(its not \"binary\"). \n",
    "                     /    |    \\ \n",
    "               Yes  /     |     \\  No   \n",
    "                   /      |      \\       \n",
    "2 Yes/3 No`s      /   4Yes|0No    \\  3Yes/2No`s   all the sum(Yes)=9\n",
    "         (sunny)🔘       🔘      🔘(rain)\n",
    "                      (overcast)\n",
    "                       \"leafnode\"\n",
    "            in these particuler case \"overcast\" is our \"leafnode\".\n",
    "            so here the for ther \"spliting\" will going to happen with\n",
    "            \"sunny\" and \"rain\" why \"overcast\" not split means my output\n",
    "            will alway be \"Yes\" only there is any \"No`s\"\n",
    "\n",
    "WHAT IS LEAFNODE ?\n",
    "\n",
    "Ans:=\n",
    "    \"leafnode\" say either you should have \"yes\" either you\n",
    "    should have \"No\"\n",
    "    \n",
    "Note:=    \n",
    "* \"splitng\" will happen \"unless and untill\" we dont get the \"leafnode\".\n",
    "   once we get the \"leafnode\" spliting will get \"stop\".\n",
    "    \n",
    "we can see \"leaf node\" but how the aplication will no the \"leafnode\"\n",
    "this is the \"leafnode\"?\n",
    "\n",
    "we need to  do some \"calculations\" whether we are  getting \"leafnode\"\n",
    "or not, so for that we have to do introduce 2 turms\n",
    "\n",
    "we check something called as \"Purity\"\n",
    "\n",
    "1.\"Purity\" \n",
    "\n",
    "Ans :=\n",
    "    \"Purity\" bassically means that whether the \"node\"/\"Tree\" is\n",
    "    \"pure_split\" or not,\n",
    "    \n",
    "     to check \"purity\" there are 2 different thing we have \n",
    "      1. Entropy\n",
    "      2. Gini index\n",
    "    \n",
    "    so we have 2 techniqes to check \"Tree\"/node \"Pure_split\" or not\n",
    "    \n",
    "Here the question raise why we are selected the \"outlook\" feature as root\n",
    "not select as root feature as any other features instead of \"outlook\" ?\n",
    "\n",
    "how the \"features\" are \"selected\" ?\n",
    "ans:=\n",
    "    1.Information gaine\n",
    "    \n",
    "    \"Information gaine\" will hepls you to select the features.\n",
    "    \n",
    "\n",
    "Note:=\n",
    "    \n",
    "whenever we get \"Leafnode\" it is \"pure_split\" \n",
    "\n",
    "whenever we have some numers of \"yes\" and some no.of \"no`s\"\n",
    "that time we bassically have a \"impure split\" \n",
    "\n",
    "Entropy is always ranging B/w [0  to  1]\n",
    "\n",
    "**Entropy :=\n",
    "    \"Entropy\" is used for to check the \"Tree\" is \"purely\"\n",
    "    \"splited\" or not means \"Purity_split\" or not\n",
    "    \n",
    "    Entropy fomula :=\n",
    "        \n",
    "    H(S)= -(P+)log2(P+) -(P-)log2(P-)  this is work for\n",
    "                                       \"binary classification\" problem\n",
    "        \n",
    "\n",
    "for \"multy classification\" where i have 3 categories so for that\n",
    "\n",
    "    H(s)= -Pc1(log2)Pc1 -Pc2(log2)c2  -Pc3(log2)c3\n",
    "    this is for \"multy classification\" problem as the categories increse\n",
    "    \"formula\" increase\n",
    "    \n",
    "    So we come to know we can solve \"binary classification\" problem\n",
    "    and \"multy class classification\" problem as well,\n",
    "    \n",
    "    Entropy value will be ranging bitween 0 to 1 but not on\n",
    "    the root node\n",
    "    \n",
    "    let say i have Tree split like this\n",
    "    \n",
    "                (6Yes\\3No`s)\n",
    "                     🔘\n",
    "                     / \\\n",
    "                Yes /   \\  No        sum of all s= 6 yes\n",
    "                   /     \\\n",
    "   (3Yes\\3No`s)c1🔘      🔘c2 (3Yes\\0No`s)('leafnode')\n",
    "     in this                     we not concider this\n",
    "    P_r(yes)=3/6                          \n",
    "    p_r(no)=3/6                   \n",
    "    3y+3n=6                     \n",
    "                        \n",
    "now how do we calculate \"Entropy\" of this?\n",
    "Ans :=\n",
    "    so its a \"Binary categories\" so it a \"binary classification\"\n",
    "    \n",
    "    H(S)= -(P+)log2( P+) -(P-)log2(P-)\n",
    "    \n",
    "    in this formula (P+) = P_r(yes)\n",
    "                    (p-) = P_r(no) \n",
    "    \n",
    "\n",
    "    3yes/3no    3              3\n",
    "    H(s) = -  ----log2(3/6) - ----log2(3/6) = ~ 1\n",
    "                6              6\n",
    "     H(s) = 1   \n",
    "        \n",
    "    So my \"Entropy\" for the \"c1\" is 1   \n",
    "    \n",
    "    H(s) = ~1 so this is very \"bad\"/worst \"impure\" split\n",
    "    \n",
    "    \n",
    " Now with respect to \"c2\" branch \"c2\" has (3Yes\\0No`s)\n",
    "\n",
    "   H(S)= -(P+)log2( P+) -(P-)log2(P-)\n",
    "\n",
    "                3              0\n",
    "    H(s) = -  ----log2(3/3) - ----log2(0/3) = 0-0=0\n",
    "                3              3\n",
    "        \n",
    "        So my \"Entropy\" for the \"c2\" is 0   \n",
    "        \n",
    "    H(s) = 0 so this is \"pure_split\" becoze this is the \"leafnode\".\n",
    "    \n",
    "let say \n",
    "\n",
    "    if 2yes/3no\n",
    "    H(s) = -(2/5)log2(2/5)-(3/5)log2(3/5) \n",
    "    H(s) = 0.97  so this is \"impure\" split\n",
    "    \n",
    "So make a graph like\n",
    "\n",
    "          H(s)\n",
    "Entropy 1--|--      🔴          P_r(+,-) probability  values ranging\n",
    "           |      *     *         b/w   0 to 1 either it can be 1\n",
    "           |    *         *        or it can be 0 \n",
    "           |   *           *\n",
    "           |  *              *       so when me P_r(0.5) and Entropy = 1  50/50\n",
    "           | *                *      my graph like this\n",
    "           |*                  * \n",
    "         0-|---------|----------|->P_r\n",
    "           0        0.5         1  \n",
    "        \n",
    "        probability value is b/w [0 to 1] 0.5 and 0.5 fair probablity\n",
    "                                          0.3 and 0.7 unfair probability\n",
    "                                         \n",
    "            \n",
    "            1,  0.5 means when i have 50/50 chance like( 3yes/3no`s)\n",
    "            2,  at H(s) there is 1 means when my \"Entropy\" value is 1 \n",
    "            \n",
    "            here based on these 2 thing my graph will be look top one\n",
    "            if i create the \"Curv\", then its looks like top one graph\n",
    " V.V.IMP           \n",
    "when  it is \"more towords\" 0 and when it is \"more towords\" 1\n",
    "that bassically means \"Purity\" of the \"Tree\" \"split\" is \"incresing\"\n",
    "that means \"Entropy\" value \"decreses\".\n",
    "  \n",
    "if \"Entropy\" value \"decreses\" that means \"purity\" of the \n",
    "\"Tree_split\" increase \n",
    "\n",
    "some examples :- if H(s) < 1 my graph will be\n",
    "\n",
    "          H(s)  ●  \n",
    "Entropy 1--|--      🔴          \n",
    "           |      *     *        \n",
    "           |    *         *       \n",
    "        ---|---*----------● *        like this is how \"Entropy\" values \n",
    "           |  *           |  *       ranges, with respect to P_r decrese\n",
    "Entropy ---|-*-●          |   *     \n",
    "value      |*  |          |    * \n",
    "here     0-|---|----------|----|->P_r\n",
    "           0   0.3       0.6   1  \n",
    "    \n",
    "    \"probability\" value is ranging b/w [0 to 1]\n",
    "    \n",
    "    simply under stand\n",
    "    \n",
    "    3yes/3no   P_r(yes) => 3/3 = 1 is probability so\n",
    "    \n",
    "    H(s) = 0 why 0 means P_r = 1 is also pointig to 0 value of \"Entropy\"\n",
    "    \n",
    "    0yes/3no    P_r(yes) => 0/3 = 0 is probability so\n",
    "    \n",
    "    H(s) = 0 why 0 means P_r(yes) = 0 is also pointig to 0 value of \"Entropy\"\n",
    "    so these both are \"Pure_spilt\",\n",
    "    \n",
    "    \n",
    "from here we just checking the \"Purity\" of the spilt.... with \"Entropy formula\"\n",
    "           \n",
    "\n",
    "***Gini Index:=\n",
    "    \"Gini Index\" is also is used for to check othe \"Tree\" is \"purely\"\n",
    "    \"splited\" or not means \"Purity_split\" or not\n",
    "    \n",
    "\n",
    "\"Gini Index\" Formula :\n",
    "                 n \n",
    "        G.I = 1- ∑ (p)²  \n",
    "                i=1\n",
    "\n",
    "this formula with respect to \"binary classification \"\n",
    "\n",
    "       1-[(P_r+)² + (P_r-)²]\n",
    "    \n",
    "     if 3 yes/3no`s                    \n",
    "    \n",
    "       = 1-[(3/6)² + (3/6)²]\n",
    "        \n",
    "       = 1-[(1/2)² + (1/2)²]\n",
    "    \n",
    "       =1-[ 1/4 +1/4] = 4+4/16= 1/2\n",
    "    \n",
    "       = 1- 1/2 = 0.5 \n",
    "    \n",
    "so we can say for \"impure_split\" with (3yes/3nos)\n",
    "my \"Gini index \" = 0.5\n",
    "\n",
    "\n",
    "  \"Gini Index\" for (4yes/8no`s)\n",
    "    \n",
    "      4/12 = 1/3      8/12 = 2/3 \n",
    "        \n",
    "       = 1-[(P_r+)² + (P_r-)²]\n",
    "    \n",
    "       = 1-[(1/3)² + (2/3)²]\n",
    "        \n",
    "       = 1- [1/9 + 4/9] = 1-5/9 = 9-5/9 = 4/9 = 0.444\n",
    "    \n",
    "    \n",
    "  \"Gini Index\" for (8yes/2 nos)    \n",
    "      8/10 = 4/5      2/10 = 1/5\n",
    "        \n",
    "       = 1-[(P_r+)² + (P_r-)²]\n",
    "    \n",
    "       = 1-[(4/5)² + (1/5)²]\n",
    "        \n",
    "       = 1- [16/25 + 1/25] = 1-17/25 = 25-17/25= 8/25 = 0.32 \n",
    "    \n",
    "    \n",
    "So \"Gini Index\" will range b/w[ 0 to 0.5] if it is 0.5 its a \"impure\" \n",
    "spilt    \n",
    "\n",
    "    \n",
    "So \"Entropy\" will range b/w[ 0 to 1] if it is 1 its a \"impure_split\" \n",
    "    \n",
    "    \n",
    "          H(s)     \n",
    "Entropy 1--|--       ●              \n",
    "           |      *     * \n",
    "           |    *        *\n",
    "Gini   0.5-|- *     🔴     *             \n",
    "Index  0.4-| *---*      *   *       \n",
    "       0.3-|*-*  |         * *\n",
    "           |**|  |           * \n",
    "         0-|*-|--|---|--------|->P_r\n",
    "           0        0.5       1 \n",
    "        \n",
    "        \"probability\" value is ranging b/w [0 to 1]\n",
    "        \n",
    "         ● is my \"Entropy\" point  its ranging b/w [0 to 1]\n",
    "            \n",
    "        🔴 is my \"Gini Index\" point and its ranging b/w [0 to 0.5]\n",
    "        \n",
    "\n",
    "        \n",
    "from here we just checking the Purity of the \"Tree\" spilt.... with\n",
    "\"Entropy\" and \"Gini Index\" formulas\n",
    "\n",
    "\n",
    "   \n",
    "    \n",
    "Which \"features\" to take to Spilt ?\n",
    "\n",
    "Here the question raise why we are selected the \"outlook\" feature as root\n",
    "not select as \"root feature\" as any \"other features\" instead of \"outlook\" ?\n",
    "\n",
    "how the \"features\" are \"selected\" ?\n",
    "ans:=\n",
    "    1.Information gaine\n",
    "    \n",
    "    \"Information gaine\" will hepls you to \"select the features\".\n",
    "    \n",
    "\n",
    "when to use \"Entropy\" and when to use \" Gini Index\" ?\n",
    "Ans :=\n",
    "    \"Gini\" should be used for \"learge Dataset\"  \n",
    "    \"Entropy\" should be used for \"smaller Dataset\"\n",
    "    \"Gini\" is quite \"efficient\" when comapare to \"Entropy\"\n",
    "    performance wise Gini is good its not use \"log\"\n",
    "    \n",
    "\n",
    "what does \"Information gaine\" do ?\n",
    "\n",
    "Ans:=\n",
    "    let say   \n",
    "    \n",
    "                \"Entropy\" ofthe\n",
    "                root is = 0.94\n",
    "                      9yes/5no\n",
    "                       /     \\\n",
    "                      /       \\ \n",
    "                     /         \\   \n",
    "                    /           \\              \n",
    "     6yes/2no,-----/             \\----->3yes/3no here \"Entropy=1\"   \n",
    "                                                      \"Gini Index=0.5\"\n",
    " \n",
    "let say i may use \"feature 2\" to do the \"split\"\n",
    "\n",
    "\n",
    "                \n",
    "                      feature 2\n",
    "                       /     \\\n",
    "                      /       \\ \n",
    "                     /         \\   \n",
    "                    /           \\              \n",
    "             c1<---/             \\--->c2   \n",
    "        feature 1                 feature 2                      \n",
    "            / \\                    /  \\\n",
    "           /   \\                  /    \\\n",
    "                \n",
    "                \n",
    "out of upper 2 \"splits\" which is \"good split\" and that is what \n",
    "\"information Gain\" bassically compares,\n",
    "             or\n",
    "out 2,3,4..n \"Split\" what is the 'good split' that is what  \n",
    "\"information Gain\" bassically compares   \n",
    "\n",
    "\n",
    "\"Information Gain\" makes sures that all the possible \"splits\" that\n",
    "are available, it will try to calucalte the \"Information Gain\" \n",
    "by combining \"Pure_spit\" into that, again \"pure_spilt\" means \n",
    "by combining this \"Entropy\" or \"Gini_Index\" into that,\n",
    "\n",
    "how this is don to get \"good spilt\":\n",
    "    \n",
    "\n",
    "                            |Sv|\n",
    "Gain(S,f1) = H(s) - ∑     --------H(Sv)  \n",
    "                   Ve_val   |s|\n",
    "                               \n",
    "                                [:.H(s)=\"Entropy\" or \"Gini Index\"]\n",
    "                                but in this particuler case let say\n",
    "                                [:.H(s)= root \"Entropy\"]\n",
    "            \n",
    "              [:.H(Sv) = is nothing bit \"Entropy\" of \"Child nodes\" ]\n",
    "                \n",
    "                 |Sv|\n",
    "               ------- -->it bassically says  in the \"child\"  \n",
    "                 |s|      \"nodes\" how many total \"categeris\" you have \n",
    "                           \"divide\" by first \"Root Categories\" feature\n",
    "                    \n",
    "let say with respect to \"feature 1\" do Split like this \n",
    "\n",
    "\n",
    "                \"Entropy\" ofthe\n",
    "                 root is = 0.94\n",
    "                      (9yes/5no)\n",
    "                       /     \\    \n",
    "                      /       \\     in child node sum(yes)=9 ,\"total\" = 14\n",
    "                     /         \\    in child node sum(no)= 5\n",
    " Entropy = 0.81     /           \\              \n",
    "   (6yes/2no),-----/             \\----->(3yes/3no) here \"Entropy=1\"   \n",
    "              \"C1\"                  \"C2\"             \"Gini Index=0.5\"\n",
    "\n",
    "calculate the  Root \"Entropy\" is root node right  (9yes/5no)\n",
    "\n",
    "H(S)= - (P+)log2(P+) - (P)log2(P-) \n",
    "\n",
    "   = -(9/14)log2(9/14) -(5/14)log2(5/14) =  0.94\n",
    "    \n",
    "    \"Entropy\" of the root is =  0.94\n",
    "    \n",
    "H(c1)= - (P+)log2(P+) - (P)log2(P-) \n",
    "\n",
    "   6yes/2no  \n",
    "    = -(6/8)log2(6/8)-(2/8)log2(2/8) =  0.81\n",
    "    \n",
    "H(c2)= - (P+)log2(P+) - (P)log2(P-)\n",
    "\n",
    " 3yes/3no  \n",
    "    = -(3/6)log2(3/6) -(3/6)log2(3/6) = 1  \n",
    "    \n",
    "    So H(c1) = 0.81 \"Entropy\" of the \"child\" say\n",
    "       H(c2) = 1    \"Entropy\"  of the \"child\" say \n",
    "        \n",
    "    and H(S) = 0.94 \"Entropy\" of the \"root\"\n",
    "        \n",
    "    So over all when you have all this thing \"Entropy\" values,\n",
    "\n",
    "                            |Sv|\n",
    "Gain(S,f1) = H(s) - ∑     --------H(Sv) \n",
    "                   Ve_val   |s|      \n",
    "    \n",
    "                            |Sv|\n",
    "                          -------- it basically means totatal no.of\n",
    "                            |s|    \"child categories\" devide by total\n",
    "                                    no.of \"root categories\"\n",
    "     \n",
    "        \n",
    "    Gain(S,f1) = 0.94 - [8/14(0.81) + 6/14(1)]\n",
    "               = 0.94 - [0.462 + 0.42] = 0.049\n",
    "        \n",
    "        \"Information Gain\" = 0.049 \n",
    "        \n",
    "Do one more split with respect to \"feature 2\" \n",
    "\n",
    "                \"Entropy\" ofthe\n",
    "                root is = ~ 0.94\n",
    "                      (9yes/5no)\n",
    "                       /     \\    \n",
    "                      /       \\     in child node sum(yes)=9 ,\"total\" = 14\n",
    "                     /         \\    in child node sum(no)= 5\n",
    " Entropy = 0.65     /           \\              \n",
    "   (5yes/1no),-----/             \\----->(4yes/4no) here \"Entropy=1\"   \n",
    "              \"C1\"                  \"C2\"   \n",
    "        \n",
    "\n",
    "H(S)= - (P+)log2(P+) - (P)log2(P-) \n",
    "\n",
    "   = -(9/14)log2(9/14)-(5/14)log2(5/14) = 0.94\n",
    "    \n",
    "    \"Entropy\" of the \"root\" is =  0.94\n",
    "    \n",
    "H(c1)= - (P+)log2(P+) - (P)log2(P-)  \n",
    "\n",
    "   5yes/1no \n",
    "    = -(5/6)log2(5/6)-(1/6)log2(1/6) =0.65\n",
    "        \n",
    "H(c2)= - (P+)log2(P+) - (P)log2(P-)  \n",
    "\n",
    "   5yes/1no \n",
    "    = -(4/8)log2(4/8)-(4/8)log2(4/8) = 1 \n",
    "    \n",
    "    \n",
    "                |Sv|\n",
    "              -------- [6/14+ 8/14](H(Sv))\n",
    "                 |s|  \n",
    "                \n",
    "    Gain(S,f1) = 0.94 - [6/14(0.65)+ 8/14(1)] = 0.089\n",
    "               \n",
    "        \"Information Gain\" = 0.089\n",
    "\n",
    "     [:.IG = \"Information Gain\"]\n",
    "    \n",
    "    \n",
    "    \"Information Gain\" for \"feature1\" = 0.049\n",
    "    \"Information Gain\" for \"feature2\" = 0.089\n",
    "        \n",
    "so  IG(\"feature2\") > IG(\"feature1\") we will be using \"feature2\"  \n",
    "to do the \"spliting information\",\n",
    "\n",
    "so like this all the \"combination\" of the \"features\" \"Information Gaine\"\n",
    "is calculated internally, by the \"Decision_Tree\" to select the \"best Split\".\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"Numerical Features\" forgot about \"Regression\" problem think if we have\n",
    "\"numerical features\" how we can \"split\" it.\n",
    "\n",
    "\n",
    "@Numerical Feature :=\n",
    "    \n",
    "    this is bassically with respect to 'independent feature'\n",
    "let say i have features like \n",
    "\n",
    "    f1         f2    opt\n",
    "    10         --     0   see here the \"dependent feature\" if having\n",
    "    15         --     1  \"fixed\" no.of \"categories\" so then we can use\n",
    "    20         --     1  'classification' problem\n",
    "    25         --     0\n",
    "    35         --     1\n",
    "    40         ---    0\n",
    "               ---    1 \n",
    "               ---->this f2 is has \"Categorical\" features  \n",
    "            \n",
    "Note :-\n",
    "    with respect \"Numerical Feature\" we \"SORT\" the feature and then \n",
    "    create \"Binary Tree\" and and then we calculate the \"Entropy\"\n",
    "    \"Gini Index\" and for the feature selection we use \"Information Gain\"\n",
    "   \n",
    "now how do you make a spilt over \"numerical features\" here \n",
    "\n",
    "so here we Introduce (\"CART\") = Classification & Regression Tree_Algorithem\n",
    "\n",
    "in CART we bassically create \"Binary_Tree\"\n",
    "\n",
    "lest make sure our independent feature is sorted \n",
    "\n",
    "              10 15 20 25 30 35 40\n",
    "    \n",
    "    \n",
    "                   (<= 10)\n",
    "                     🔘\n",
    "                     / \\\n",
    "                Yes /   \\  No        \n",
    "                   /     \\\n",
    "       (0yes/1no)🔘      🔘(4yes/2no)\n",
    "                    \n",
    "                    \n",
    " after \"spilting\" we calculate \"Entropy\" or \"Gini Index\" and\n",
    "to find out the \"Information Gain\"\n",
    "\n",
    "                  \n",
    "similarly            \n",
    "let say i ve got some \"information gaine\"\n",
    "\n",
    "and then for my next \"Decision_tree\" like below \n",
    "\n",
    "                   (<= 15)\n",
    "                     🔘\n",
    "                     / \\\n",
    "                Yes /   \\  No        \n",
    "                   /     \\\n",
    "       (1yes/1no)🔘      🔘(3yes/2no)\n",
    "                \n",
    "                \n",
    "    Again here we calculate \"Entropy\" or \"Gini Index\" and\n",
    "    \"Information Gain\" will gets calculated\n",
    "                \n",
    "                \n",
    "and then for my next \"Decision_tree\" like below \n",
    "\n",
    "                   (<= 20)\n",
    "                     🔘\n",
    "  greter value       / \\    less then values here \n",
    "                Yes /   \\  No        \n",
    "                   /     \\\n",
    "       (2yes/1no)🔘      🔘(2yes/2no)\n",
    "                \n",
    "        Again here we calculate \"Entropy\" or \"Gini Index\" and\n",
    "        \"Information Gain\" will gets calculated\n",
    "        \n",
    "and then for my next \"Decision_tree\" like below\n",
    "        \n",
    "                   (<= 30)\n",
    "                     🔘\n",
    "  greter value       / \\    less then values here \n",
    "                Yes /   \\  No        \n",
    "                   /     \\\n",
    "       (2yes/2no)🔘      🔘(2yes/1no)\n",
    "        \n",
    "        \n",
    "        Again here we calculate \"Entropy\" or \"Gini Index\" and\n",
    "        \"Information Gain\" will gets calculated\n",
    "        \n",
    "        see all the \"Tree\" are maked like \"Binary Trees\" format\n",
    "        b-coz they introdused \"CART\"\n",
    "                \n",
    "                \n",
    "so here with respect all  \"Decision_Tree\"s \"Information Gain\" will get\n",
    "Calculated  and which ever will be the \"highest\" one \"Information Gain\"\n",
    "that will selected, \n",
    "\n",
    "    \n",
    "how do we make a split this so that is where \"CART\"   \n",
    "\"Algorithem\" will come in cart we basically create \n",
    "binary tree\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "324fca03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.333333333333332"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(20+24+28+14+16+20)/6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2f1f8b",
   "metadata": {},
   "source": [
    "# Decision tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28d05e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Decision_Tree\" Regressor :=\n",
    "\n",
    "    \"Decision_Tree\" \"Regressor\" specifically for solving \"Regresson\" problem\n",
    "    statements\n",
    "\n",
    "  Ex :-\n",
    "    \"Size of the House\" is based on \"Price of the House\"\n",
    "\n",
    " \n",
    "i have features like\n",
    "\n",
    "    f1      f2     price        i need to solve this \n",
    "    --      --      20 }         with \"Decision_Tree\"\n",
    "    --      --      24 }->c1\n",
    "    --      --      28 } \n",
    "    \n",
    "    --      --      14 }         \n",
    "    --      --      16 }->c2\n",
    "    --      --      20 }\n",
    "    \n",
    "    let say the split will be\n",
    "    \n",
    "            (20+24+28+14+16+20)/6 = 20.3 = \"mean\"\n",
    "            when we say \"mean\" we can use 'MSE'/\"MAE\" as my loss/\"Cost_fuction\" \n",
    "            we caluculate \"MSE\"/\"MAE\" by \"subtracting\" each & every elemennt of \n",
    "            \"output\" with the \"mean\" and we can get \"MSE\"/\"MAE\".\n",
    "            \n",
    "            \n",
    "                       f1 (here i put \"Mean\" of all o/p feature values)\n",
    "                      🔘\n",
    "                      / \\     \n",
    "                 Yes /   \\  No        \n",
    "                    /     \\\n",
    "\"Mean\"(20,24,28)c1🔘      🔘c2\"Mean\"(14,16,20) here also we calulate\n",
    "here \"mean\" is my opt                           \"MSE\"\n",
    "        \n",
    "    \n",
    "if we take the \"Mean\" of all the \"opt feature\" it will be \"output\" in \n",
    "the \"root node\" \n",
    "\n",
    "when we calculate \"Mean\" what will be my \"loss function\"\n",
    "\n",
    "ans:=\n",
    "    \"MSE\"[mean_squared_error] or \"MAE\"[mean_absolute_error] \n",
    "    i can use as my \"Cost_function\" \n",
    "    \n",
    "         1\n",
    " MSE = ----- ∑ (yi-^yi)²  [:.^yi = is my \"mean\"]\n",
    "         2m               [:.^y = \"predicte value\" bassically means \"mean\" value]\n",
    "\n",
    "    \n",
    "    so as we find out  \"mean_squared_error\"[MSE] at \"Every node\"  as we \n",
    "go dowd the \"mean_squared_error\" will be \"Reduse\", that is what we do\n",
    "in \"Decision_Tree\" Regressor\n",
    "\n",
    "\n",
    "As we go down untile  \"Keep on check\" at the place of \"leafnode\". so that\n",
    "Time Complexity will \"increase\" in Case of \"Decision_Tree\" Algorithem\n",
    "Sothat is why \"Rondom_Forest\" is introdused\n",
    "\n",
    "another Example :-\n",
    "    \n",
    "                                  \n",
    "   f1               opt          \n",
    "    2                10  --> node 1\n",
    "   ----------------------- \n",
    "    4                12  -- node 2 \n",
    "    ---------------------\n",
    "    6                8\n",
    "    8                9\n",
    "    10               6\n",
    "    20               5 \n",
    "    40               15 \n",
    "    \n",
    "my split will be like\n",
    "                     so here all o/p \"Mean\" o/p= 9.2\n",
    "                    (<=2)f1 (here i put \"Mean\" of all o/p feature values)\n",
    "                      🔘    \n",
    "                      / \\     \n",
    "                 Yes /   \\  No        \n",
    "     \"Mean\"=10/1=10 /     \\\n",
    "        output= 10🔘      🔘\"Mean\"(12,8,9,6,5,15) here also we calulate\n",
    "                            \\    \"Mean\" O/p = 9.16            \n",
    "                             \\\n",
    "                              \\ \n",
    "this is the entire split       \\              (12+8+9+6+5+15)/6 = 9.16\n",
    "will happen with respet this    \\ (<=12)  \n",
    "Decision_Tree                   🔘\n",
    "and this \"Information Gain\"    /  \\\n",
    "will be something             /    \\\n",
    "                             /      \\\n",
    "            \"Mean\"= 12/1=12 /        \\\n",
    "                      (12)🔘         🔘(8,9,6,5,15)  8+9+6+5+15/5=8.3 \n",
    "                                        \"Mean\"=8.3\n",
    "                            \n",
    "onther \"split\" like say 2 record if i take\n",
    "\n",
    "\n",
    "            mean of 2nd node = \"9.28\"\n",
    "                    (<=4)f1 (here i put \"Mean\" of all o/p feature values)\n",
    "                      🔘    \n",
    "                      / \\     \n",
    "                 Yes /   \\  No        \n",
    "     10+12/2 = 11   /     \\\n",
    "\"Mean\"=11  (10,12)🔘      🔘(8,9,6,5,15) Mean = 8.6 here\n",
    "                          / \\\n",
    "                         /   \\\n",
    "                        🔘   🔘 like this \"Decision_Trees\" will gets\n",
    "                                  ceated and which ever \"Decision_Tree\" \n",
    "                                  has the \"maximum\" \"Information Gain\" \n",
    "                                  that will be get Selected.\n",
    "        \n",
    "                        \n",
    "\n",
    "example:= calculating [\"MSE\"]  it will decrese as we go down\n",
    "    \n",
    "     (yi-^yi)²  = (10 - 11)²+(12-11)² = 2 so like that this will be\n",
    "                                   decrese when compare to root\n",
    "                                   \"mean\" 9.8 > 2\n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "                                 \n",
    "\n",
    "so as we find out  \"mean_squared_error\"[MSE] at \"Every node\"  as we \n",
    "go dowd the \"mean_squared_error\" will be \"Reduse\", that is what we do\n",
    "in \"Decision_Tree\" Regressor\n",
    "\n",
    "\n",
    "As we go down untile  \"Keep on check\" at the place of \"leafnode\". so that\n",
    "Time Complexity will \"increase\" in Case of \"Decision_Tree\" Algorithem\n",
    "Sothat is why \"Rondom_Forest\" is introdused\n",
    "                            \n",
    "\n",
    "\n",
    "\n",
    "what is the maximum \"Information Gaine\" ?\n",
    "\n",
    "Ans:=\n",
    "    which ever \"Decision_Tree\" has the \"maximum information\"\n",
    "    that will get selected \n",
    "    \n",
    "    if i try to find out \"MSE\" at \"every node\" as we go down \n",
    "    the \"MSE\" will be \"reducing\" that is what we do in Decision\n",
    "    tree Regressor\n",
    "    \n",
    "    \n",
    "How features are selected in \"Decision_Tree Regression\" ?\n",
    "\n",
    "\n",
    "\"feature\" is get selcted based on \"MSE\"/\"MAE\"(Information Gain) In\n",
    "\"Decision_Tree\" Regression \"Information Gain\" means [\"MSE\"] the \"loss_function\"\n",
    "in case of \"Decision_Tree Regression\"\n",
    "\n",
    "    \n",
    "    \n",
    "@Decision Tree :=\n",
    "    \n",
    "    if we create Decision_Tree \"completly to depth\" there will be \n",
    "    chances of occuring \"Overfitting\" becoz we are not checking\n",
    "    all the \"Test_data\".So Decision tree will lead to \"Overfitting\"\n",
    "    \n",
    "    \n",
    "     \"Overfitting\" in the sence \"low Bias\" ,\"high Variance\"\n",
    "    means the \"Training_Error\" is less \"Testing_Error\" is \"high\"\n",
    "    \n",
    "    \n",
    "                      🔘    \n",
    "                      / \\  \\   \n",
    "                     /   \\  \\        \n",
    "                    /     \\\n",
    "                  🔘      🔘\n",
    "                        / / \\\n",
    "                         /   \\\n",
    "                      / 🔘   🔘 \n",
    "                     /  /      \\\n",
    "                       /        \\\n",
    "                      /          \\\n",
    "                    🔘           🔘\n",
    "\n",
    "    \n",
    "\n",
    "how do we fix this \"overfitting\" in \"Decision_Tree\" ?\n",
    "\n",
    " Ans := there are 2 things \n",
    "         1. \"Post_pronning\" \n",
    "         2. \"Pri_pronnig\" \n",
    "        \n",
    "        \n",
    "1.\"Post_pronning\" :=\n",
    "    ans:-\n",
    "    \n",
    "    in \"Post_pronning\" we constract the \"Decision_Tree\" to the \"depth\" \n",
    "    then we try to cut the that \"Tree\", after cutting the Tree the \n",
    "    \"upper\" part of the side is calles as \"Max_depth\",\n",
    "    \n",
    "    when we have to CUT the Decision_Tree ?\n",
    "    Ans:-\n",
    "        \"Decision_Tree\" its some \"Quber some\" proses so when you kwon\n",
    "        the node like splited up to 70 to 80 persent. then you can CUT\n",
    "        Over there to prevent \"Overfitting\". so for utting we use\n",
    "        \"parameter\" called \"max_depth\"\n",
    "        \n",
    "        \n",
    "    if your \"initially\" choosing \"Max_depth\" is this much/something\n",
    "    \"value\" that bassically become \"Pri_pronning\".\n",
    "    \n",
    "    when you have \"many featuters\"  \"post_proning\" should never be done\n",
    "    you should go with \"pre_proning\"\n",
    "        \n",
    "        [:.Max_depth = its \"hyper parameter\"]\n",
    "        \n",
    "         we  will use \"pri_proning\" in (GridSearchCV)\n",
    "        \n",
    "        in the \"initial\" stage only you saying \"some specific level\" you\n",
    "        should go and the Create \"Decision_Tree\"\n",
    "        \n",
    "        \"max_depth\" deside the \"depth\" and \"height\" of the tree\n",
    "        \n",
    "        So we use \"Post_prooning\" with repect \"smaller dataset\"\n",
    "        when we have Large amount of dataset then we use \"Pre_prooning\"\n",
    "        \n",
    "        \n",
    "NOTE :-\n",
    "        when you have many features never do \"post_pronning\"\n",
    "        it should be \"pre_ponnig\".b-coz it takes time to constract\n",
    "        \"Decision_Tree\"to its depth.\n",
    "        \n",
    "        use \"Post_prunning\" when you have \"smaller dataset\"\n",
    "        use \"Pri_prunning\" when you have \"lerger dataset\"\n",
    "        \n",
    "        \n",
    "        \n",
    "--------------------------------------------------------------------------\n",
    "        \n",
    "   what all parameters are there in Decision_Tree classification ?\n",
    "\n",
    "   1.Criterion = \"gini\",\"entropy\",\"log_loss\"\n",
    "    \n",
    "    \"log_loss\" in \"Decision_Tree\". \n",
    "    here \"log_loss\" logistic \"Cost_function\" this is specifically\n",
    "    \"Binaryclass\".\n",
    "    \n",
    "                \n",
    "    2.Spilitter = 'best','rondom'\n",
    "          \n",
    "          if i select parameter \"best\" it will select \"Infomation Gain\".\n",
    "          to choose Decision_Tree,\n",
    "            \n",
    "          if i select parameter \"rondom\" no \"Infomation Gain\" will select\n",
    "    \n",
    "     3.Max_depth = it desides the \"depth\" and \"hieght\" of the \"Tree\"\n",
    "                   # \"Pri_pronning\" #\"Post_pronning\".\n",
    "            \n",
    "     4.Minimum_sample_split = the minimum no.of the samples required to \n",
    "     split an internal node .\n",
    "    \n",
    "            \n",
    "    4.Minimum_sample_leaf =  the minimum number of the samples required to \n",
    "     be at a \"leafnode\".\n",
    "    \n",
    "    5.Max_features :{'auto','sqrt','log2'} 'auto' and 'sqrt' are same\n",
    "        \n",
    "       Max_features the number of \"features\" consider, when looking for\n",
    "       the \"best split\"\n",
    "                  when i am doing spliting atleast look for  10  feature\n",
    "                20 feature now how we calculate the formula.\n",
    "                \n",
    "            when we select  Max_features = \"auto\"   \n",
    "            sqrt(n_features) if no.of featurs 4 before split, it will look 2\n",
    "            log2(n_features)= log2(4) = 2.0\n",
    "            \n",
    "    6.class_weight := waitght is associated with classes in the form of \n",
    "                    class_label = weight\n",
    "            \n",
    "    Example :-\n",
    "         if we have a \"imbalence dataset\" if we apply \"class_weight\" to\n",
    "        the smaller dataset or smallar datapoint that will become like a \n",
    "        \"balenced\" dataset \n",
    "        \n",
    "   what all parameters are there in Decision_Tree regressor ?\n",
    "\n",
    "ccriterion :{\"squared_error\", \"friedman_mse\", \"absolute_error\",\n",
    "             \"poisson\"} default=”squared_error”\n",
    "            \n",
    "-------------------------------------------------------------------------\n",
    "\n",
    "          \n",
    "\"Overfitting\" := \n",
    "    \n",
    "here \"Traning_Error\" is \"less\", here when ever we have \"data points\"\n",
    "with respect to \"Test_data\" my \"Test_Error\" will be \"high\"\n",
    "\n",
    "when ever \"Traning_Error\" is \"less\" obviously we use \"low_Bise\" with\n",
    "this combination you will definatly have \"high_variance\"\n",
    "\n",
    "that basically means your \"model\" performing good, it is very goodly\n",
    "fitted for the \"Trianing_data\", means\"Trianing_data\" Accuracy is \n",
    "very \"high\" but for the \"Test_data\" Accuracy is very very \"low\" this\n",
    "condition called as \"Overfitting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "26539c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.285714285714286"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "65/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d32e7710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log2(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "63ef0883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.166666666666666"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(12+8+9+6+5+15)/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efd8f35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2b38c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2e331eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8181818181818182"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9/11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa844ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d465ead",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
