{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e49e1a8",
   "metadata": {},
   "source": [
    "# 1. Ridge and Lasso Regration :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66062271",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Ridge and Lasso Regration :\n",
    "    in this Ridge and Lasso Regration we discouss about key\n",
    "    words called like \"Overfitting\",\"Bias\",\"Variance\"\n",
    "    \"Underfitting\"\n",
    "    \n",
    "    \n",
    "\n",
    "what is Ridge Regration and why \n",
    "do you use all to regularisation  ? \n",
    "\n",
    "Ans := \n",
    "    it is just to reduse \"Over_fitting\" \n",
    "\n",
    "lets go back to the \"Simple Linear Regration\"\n",
    "      \n",
    "       MODEL : 1            MODEL : 2          MODEL : 3\n",
    "              \n",
    "     |          *          |          *          |         *\n",
    "     |      *              |       *             |      *  \n",
    "     |    *                |    *                |    *\n",
    "     |  *                  |  *                  |  *\n",
    "     | *                   | *                   | *  \n",
    "     |*                    |*                    |*\n",
    "    -*------------        -*---------------    --*------------- \n",
    "     |                     |                     |\n",
    "    let say model:1 have  model:2 have           model:3 have\n",
    "    data points           data points            data points have \n",
    "    litle distance        some little close      on the \"best fit line\"\n",
    "    to \"best fit line\"    close \"best fit line\"  here \"Trainning Error\" near\n",
    "    if data points are    here \"Trainning Error\"  to less\n",
    " far so \"Trainning_Error\"  litle low\n",
    "    is \"high\"\n",
    "    \n",
    "    assume in all this \"3 model\" we seeing  \"Train_data\" datapoint\n",
    "    \n",
    "------------------------------------------------------  \n",
    "\"Trainning Error\" is nothing bt the difference B/W \"Train_data\" points\n",
    "and \"Best fit line\" points is called \"Trainning Error\"( the difference )\n",
    "\n",
    "\"Testing Error\" is nothing bt the difference B/W \"Test_data\" points\n",
    "and \"Best fit line\" points is called \"Testing Error\"\n",
    "-----------------------------------------------------\n",
    "    \n",
    "    let say all the 3 MODELs \"Regression\" problems\n",
    "\n",
    "  MODEL : 1 :\n",
    "    \n",
    "\"Underfitting\" :=\n",
    "        whenever the \"Trainning Error\" is \"high\" that condition \n",
    "        basically called as \"Underfitting\", that basically means\n",
    "        my \"model\" is not being Trained propely with the \"Trainning_data\"\n",
    "        \n",
    "        now in this model:1 with respect to \"Test_data\" points it may\n",
    "        be near to \"Best fit line\" it not may be near to \"Best fit line\",\n",
    "        so i can have this both. So we use \"low_Variance\" also and \n",
    "        \"high_variance\" also eithe one of this get satisfy. but with\n",
    "        respect to \"Train_data\" the \"high_Bias\" always be there\n",
    "        \n",
    "\n",
    "  1.Bias := \n",
    "    whenever we talk about \"Trainning_data\" than we bassically \n",
    "     talk about or use  \"Bias\"\n",
    "        \n",
    "  2.variance :=\n",
    "    whenever we talk about \"Test_data\" we are defnatly use  \n",
    "    \"Variance\"  here also \"2 combinations\" like \"low_variance\"\n",
    "    and \"high_variance\" \n",
    "    \n",
    "    \n",
    "    \n",
    "Note := if the \"Trainning_error\" is \"high\" then i say it has\n",
    "        \"high_Bias\"\n",
    "        \n",
    "        if the \"Trainning_error\" is \"low\" then i say it has\n",
    "        \"low_Bias\"\n",
    "    \n",
    "    \n",
    "    for MODEL:1: we have conditions like\n",
    "    \n",
    "    1. Trainig_data     2. Test_data\n",
    "      \"high_Bias\"       \"low_variance\"\n",
    "                        \"high_variance\"\n",
    "    \n",
    "Now\n",
    "--------------------------------------------------------\n",
    "why do we have this \"Trainig_data\" ,\"Test_data\" ?\n",
    "            or\n",
    "why we are doing \"Train\" and \"Test\" split our dataset?\n",
    "\n",
    "Ans:=\n",
    "    whenever i have a \"data_set\" we split our \"dataset\" into\n",
    "    \"Train\" and \"Test\"    \n",
    "    \n",
    "   the ression we split it with the help of \"Trainning_data\" we \n",
    "   will \"train\" our \"model\",\n",
    "    \n",
    "   with the help of \"Test_data\" we will check the \n",
    "  \"performance of the model\"/verify our Model  \n",
    "\n",
    "Train_Data --->    we \"Train our Model\"\n",
    "Test_Data -------> check the \"Perfermance of the Model\"\n",
    "\n",
    "NOTE:=\n",
    "    \"Test_data\" is also called as \"Validation_data\" also\n",
    "    \n",
    "------------------------------------------------------------  \n",
    "\n",
    "MODEL : 2 :\n",
    "                        Train_data datapoint\n",
    "             |          *               \n",
    "             |       *            \n",
    "             |    *                \n",
    "             |  *                  \n",
    "             | *                \n",
    "             |*                    \n",
    "        -----*---------------    \n",
    "             |      model:2 have look like above & data points                         \n",
    "                    some little close close \"best fit line\" assume\n",
    "                    so here will say \"Trainning Error\" litle getting reduced\n",
    "                           \n",
    "    \n",
    "Generalized Model :=  \n",
    "    \n",
    "    if your \"Trainning_Error\" is \"less\" which has \"low Bias\" \n",
    "    \n",
    "    when ever we have \"data points\" with respect to \"Test_data\"\n",
    "    then obviously \"Error\"(difference) will be less, so i will\n",
    "    say \"low Bias\" and \"low Variance\",this is not a \"Underfiting\"\n",
    "    not a \"Overfitting\" model, which is basically called as \n",
    "    \"Generalized Model\", we actually want this kinda of model\n",
    "    where as \"Trainig_Error\" is less and \"Test_Error\" is also\n",
    "    less this is called as \"Generalized Model\"\n",
    "    \n",
    "    it is basically being able to treet well with the \"Trainig_Data\"\n",
    "    and with \"Test_data\" also, \n",
    "    \n",
    "    if my \"Test_Error\" is \"high\" then ill  be use \"high_variance\"\n",
    "    \n",
    "    for MODEL :2 we have conditions like\n",
    "        \n",
    "    Train_Data               Test_Data\n",
    "    \"low_Bias\"              \"low_variace\"\n",
    " \n",
    "\n",
    " here \"x_axis\" ,\"y_axis\" can be any \"feature\" which are \"indipendent\" \n",
    "to each other \n",
    "    \n",
    "MODEL : 3 :\n",
    "        \n",
    "       |         * \n",
    "       |      *  \n",
    "       |    *\n",
    "       |  *\n",
    "       | *  \n",
    "       |*\n",
    "     --*------------- \n",
    "       |\n",
    "    model:3 assume  data points have on the \"best fit line\"\n",
    "    so obvisouly here \"Trainning Error\" is \"less\" \n",
    "\n",
    "\"Overfitting\" := \n",
    "    \n",
    "here \"Traning_Error\" is \"less\", here when ever we have \"data points\"\n",
    "with respect to \"Test_data\" my \"Test_Error\" will be \"high\"\n",
    "\n",
    "when ever \"Traning_Error\" is \"less\" obviously we use \"low_Bise\" with\n",
    "this combination you will definatly have \"high_variance\"\n",
    "\n",
    "that basically means your \"model\" performing good, it is very goodly\n",
    "fitted for the \"Trianing_data\", means\"Trianing_data\" Accuracy is \n",
    "very \"high\" but for the \"Test_data\" Accuracy is very very \"low\" this\n",
    "condition called as \"Overfitting\"\n",
    "\n",
    "  for MODEL :3 we have conditions like\n",
    "\n",
    "     Train_Data                Test_data\n",
    "     \"low_Bias\"              \"high_variace\"\n",
    "\n",
    "    \n",
    "    \n",
    "2 .Ridge and Lasso Regration Algorithem :=\n",
    "    \n",
    "       Y\n",
    "       |           let say i have \"simple linear regretion\"\n",
    "       |     /      x & y one input and one depen feature\n",
    "       |    *      let say i have 2 data point with respect to\n",
    "       |   /       my \"linear regretion\" will create \"best fit line\"\n",
    "       |  *        like this,\n",
    "       |/\n",
    "     --*------------- X  \n",
    "       |\n",
    "        \n",
    "    if \"best fit line\" on \"data points\" then my \"Cost_funcion\" value\n",
    "    will definetly 0\n",
    "    \n",
    "\"Y=mx+c\"         \n",
    "hθ(x) =θ0 + θ1x \n",
    "\n",
    " Cost_function :=loss fun [MSE]\n",
    "        \n",
    "                     1    m \n",
    "      J(θ0 , θ1) = -----  ∑ (hθ(xi) - yi)²\n",
    "                     2m  i=1      \n",
    "        \n",
    "    hθ(xi) - yi = \"predicted_point\"-\"real_point\" = \"Error\"(difference)\n",
    "    so upper data point will on top of best fit line so \"Error\"(difference)\n",
    "    is Obviously 0  \"Cost_function\" = 0\n",
    "     \n",
    "\n",
    "    this is my \"Overfitting\" Condition\n",
    "    why i am saying \"Overfitting\" Condition b-coz with respect\n",
    "    to \"Test_data\" points my \"Test_error\" will \"high\"\n",
    "    \n",
    "       Y\n",
    "       | * *   /    \n",
    "       |  *   *trainig_data point   \n",
    "       | *   /    *\n",
    "       |    *   *  *Testing_data point\n",
    "       |  /   * * *\n",
    "       | /\n",
    "     --|/------------> \n",
    "       |\n",
    "    \n",
    "    here i have \"low bais\" and \"high Variance\" condition i have\n",
    "    here \"low bais\" basically means \"Trainig_Error\" is \"less\"\n",
    "    \"high Variance\" basically means \"Test_error\" is \"high\"\n",
    "    \n",
    "    \n",
    "now how can we fix \"Overfitting\" condition problem?\n",
    "\n",
    "     we fix this kind of problems with \"Ridge\" and \"Lasso\"\n",
    "    \"Regration\" Algorithems\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "Ridge Regration(\"L2 regularisation\")?\n",
    "\n",
    "    Ridge Regration is a kind of \"regration\" which is spesifically\n",
    "    call it as \"L2 regularisation\". whenever i have \"overfitting\"\n",
    "    condition i can use either one of them that is \"ridge\" or \"lasso\"\n",
    "    and try to solve it \n",
    "    \n",
    "what does Ridge regretion bassically do ?\n",
    "\n",
    "let sey my data points looks like\n",
    "\n",
    "\n",
    "  Y\n",
    "  |       /      V.V.IMP\n",
    "  |      *  \n",
    "  |     /         \n",
    "  |    *          if \"best fit line\" on \"data points\",my \"Cost_funcion\" will be 0 \n",
    "  |  /             if \"Cost_funcion\" is 0,so in \"liner_regretion\" that\n",
    "  | /              basically means we directly reach the \"Global minima\" \n",
    "--|/------------>x after reaching \"Global minima\" we are not move the \"best fit line\",\n",
    "                   but we know that this will definatly an \"Overfitting\"\n",
    "                   \"Scenario\" we not stop over here we Bring \"Ridge Regration\"\n",
    "                   like by adding component \" λ (slope)²\"\n",
    "            \n",
    "            \n",
    "    go back to \"cost_function\" \n",
    "    \n",
    "   Cost_function :=\n",
    "                     1    m \n",
    "      J(θ0 , θ1) = -----  ∑ (hθ(xi) - yi)²\n",
    "                     2m  i=1\n",
    "        \n",
    "   Cost_function of \"Ridge regration\"  :=\n",
    "    \n",
    "                     1    m \n",
    "      J(θ0 , θ1) = -----  ∑ (hθ(xi) - yi)² + λ (slope)²\n",
    "                     2m  i=1  \n",
    "        \n",
    "        [:. λ = hyper parameter ]\n",
    "        [:. Slope = coffiescent (\"θ1\")]\n",
    "        \n",
    "        what is coffiescent ?\n",
    "        \n",
    "        soppose this is my eqn hθ(x) = θ0 + θ1x\n",
    "        \n",
    "           (\"θ1\" is basically my \"Slop\")\n",
    "            \n",
    "for \"multy linear regretion\" it will be if i have 2 variables\n",
    "    hθ(xi) =θ0 + θ1x1 + θ2x2 + θ3x3\n",
    "            \n",
    "            θ1 , θ2, θ3 are my \"coffiescents\" or \"Slops\"\n",
    "            \n",
    "            so (slope)² = (θ1+θ2+θ3...)² b-coz we are \"sumationing\" it \n",
    "            \n",
    "    lets say   λ = 1  , Slope(θ1) = 1.5\n",
    "    \n",
    " NOTE := \n",
    "    this \"cost_function\" is \"continueos iteration proces\" to reduce\n",
    "    this \"cost_function\". if i have \"best fit line\" then i ll be getting\n",
    "    \"cost_function\" = 0  if cost_function=0 in \"linear Regression\" that\n",
    "    bassically  means we are directly reach the \"Global minima\". so after\n",
    "    reaching \"Global minima\" we are not move the \"best fit line\" ,but if\n",
    "    we know that this will definatly an \"Overfitting\" Scenario So \n",
    "    we not stop over here we Bring \"Ridge Regration\"\n",
    "    like by adding component \" λ (slope)²\" to \"Cost_function\"\n",
    "    \n",
    "    b-coz of this component\" λ (slope)²\" my \"Trainig_data\" will not stop \n",
    "    \n",
    "    lets say   λ = 1 Slope(θ1) = 1.5\n",
    "    \n",
    "        = 0 + (1)(1.5)²\n",
    "        = 2.25 does this means that the algorithem needs to train \n",
    "               more, so that it \"reduses\" this value from 2.25\n",
    "               something else less to reach the \"global minima\"  \n",
    "        \n",
    "        \n",
    "    our main aim is to reach the \"global minima\" that basically  means my\n",
    "    algorithem thinks that ok there is still some value in my \"cost_function\"\n",
    "    /'loss_fun' so i need to keep \"reducing\"/\"decrese\" it,so i will change my\n",
    "    (θ0 , θ1) values and if i have my 2nd \"best bit line\" got like this\n",
    "            \n",
    "       Y\n",
    "       |    /    \n",
    "       |   /\n",
    "       |  /    \n",
    "       | /     \n",
    "       |/  \n",
    "       / \n",
    "      /|\n",
    "     / | \n",
    "     --|-------------> x\n",
    "       |           \n",
    "        \n",
    "  Obviously there will be some \"Error\"(difference) with respect to \n",
    "\"predicted_ponits\" and \"real_points\" but it will some small or near to\n",
    "0 value but not exactly 0\n",
    "        \n",
    "   if i change my θ0 , θ1 to find out \"best fit line\" \n",
    "\n",
    "     Slop_decrese = Slop_decrese if it is comming down\n",
    "     Slop_increse = Slop_increse if it is going up\n",
    "        \n",
    "    what is Slop ?\n",
    "    \n",
    "        with respect to the \"unit movment\" in the \"x_axis\"what\n",
    "        is the movement with respect to \"y_axis\" that movement\n",
    "        is basically shows \"Slope\n",
    "\n",
    "         = small .no +(1)(0.9)²\n",
    "         = ~ 0.81 value is \"reduced\" so this is the \"best fitline\"\n",
    "         compare to previous one it got Redused\n",
    "            \n",
    "            ~ 0.81 < 2.25\n",
    "            \n",
    "            So  here 0.81 line(best fit line) is better then 2.25\n",
    "            line(best fit line), with respect to \"new data point\"\n",
    "            \n",
    "what exactly ridge is doing ?\n",
    "\n",
    "      Ridge is making sure that even though my \"Best fit line\" set to\n",
    "      \"data point\" its not going to stop \"Traning the model\"Ridge going to\n",
    "       add penulty\" λ (slope)²\"\n",
    "\n",
    "        λ (slope)² this is called as \"penulty\" and Ridge gonna make \n",
    "        sure that atleast we come to a conclussion so that we get a \n",
    "        better line then \"previous\" \"best bit line\", so that its looks good\n",
    "        for the \"Training_data\" and \"Test_data\". so in this way we will try\n",
    "        to find out the \"best fit line\", what ever \"cost_function\"/\"loss_fun\"\n",
    "        we get as a \"minimal value\" we will keep on iterating and changing\n",
    "        this  θ0 , θ1 \n",
    "        \n",
    " \n",
    "(IMP) Relation_ship B/W  \"λ\" and  \"(slope)²\" := \n",
    "    \n",
    "    concider this \"λ\" in \"Ridge_regression\" we have \"alpha\" as \n",
    "    \"parameter\" in practice its nothing but same how we are adding penalty \n",
    "    like we have adding \"λ (Slop)² \" \"alpha\" its i Penalty\n",
    "    \n",
    "let say i have dataset like \"height\",\"weight\"\n",
    "\n",
    "    y-axis \n",
    "       |    */    \n",
    "       |    /*\n",
    "       |   /\n",
    " height|  /* \n",
    "       |*/  \n",
    "       |/\n",
    "     --*-------------   x-axis\n",
    "       |      weight       \n",
    "               \n",
    "            \n",
    "in \"cost_function\" and \"Ridge_regration\" if\n",
    "i keep on increase \"λ\"   \n",
    "\n",
    "lets say λ = 0                  λ =10                 λ =40  \n",
    "                                                              *\n",
    "     |                     |               *    |             *\n",
    "     |                     |              *     |            *  \n",
    "     |*            *       | *           *      |*         *\n",
    "     | *          *        |  *         *       | *      *\n",
    "     |  *        *         |   *      *         |    •   \n",
    "     |     * • *           |      •             |     \n",
    "    -|-|------------      -|--|-------------  --|--|----------- \n",
    "     | 0                   |  0                 |  0\n",
    "\n",
    "    \n",
    "    see here \"Global minima\" going near to 0 if i increase \"λ\" value\n",
    "    but it not Exactly 0\n",
    "    \n",
    "\n",
    "how and how much \"λ\" value will get selected ?\n",
    "\n",
    "NOTE :=\n",
    "    different different \"λ\" values will get initialized\n",
    "    \"λ\" value will a \"hyper parameter\" it will get selected\n",
    "    by \"Hyper_Parameter_Tuning\"  \"one sutable\"  \"λ\" value will\n",
    "    get selected so that i get a line which make sure that\n",
    "    \"overfitting\" condition may satisfied\n",
    "    \n",
    "note := 1.when \"λ\"value is increasing \"slope\" is \"decreasing\" \n",
    "        2.\"global minima\"getting pussed near to 0, but it is not\n",
    "        exactly 0 ,\n",
    "        \n",
    "why we are doing this ?\n",
    "        we doing this for to reduce \"overfitting\", bedcoze we will\n",
    "        be getting the proper line/\"best fit line\", and consedering\n",
    "        the Penalty in mind.we need to get a \"better line\" then previous\n",
    "        \"best bit line\" so that its works good for the \"Training_data\" and\n",
    "        \"Test_data\" means the \"Train_Error\" and \"Test_error\" is less\n",
    "        \n",
    "        if θ1 = coffiescent = 0 \n",
    "        \n",
    "         hθ(x)=θ0 + θ1x ---> we are neglcting \"θ1x\" this feature\n",
    "          but here(in Reidge regretion) we are not getting 0 here\n",
    "         we are getting near to 0, this way we try to reduce \"overfitting\"\n",
    "            so that  we should keep on changing \"Best fit line\" with\n",
    "            respect to the \"θ0\" & \"θ1\" value \n",
    "        \n",
    "{IMP}what is ridgedge regration and why \n",
    "     do you use \"L2 regularisation\"  ?    \n",
    "     Ans := \n",
    "        it is just to reduse \"Over_fitting\"   \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b824fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.9**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79e882d",
   "metadata": {},
   "source": [
    "# 2. Lasso Regration (L1 Regularisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74b2dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Lasso Regration (\"L1 regularisation\")\n",
    "\n",
    "\"Lasso_Regration\" is a kind of \"regration\" which is spesifically\n",
    "call it as \"L1 regularisation\".it is done on \"Training_data\".\n",
    "\n",
    "\n",
    "what is the changees B/W Ridge and Lasso ?\n",
    "Ans :=\n",
    "    only the \"Cost_function\" will getting changed\n",
    "\n",
    "    go back to cost_function \n",
    "    \n",
    "   Cost_function :=\n",
    "                     1    m \n",
    "      J(θ0 , θ1) = -----  ∑ (hθ(xi) - yi)²\n",
    "                    2m   i=1\n",
    "        \n",
    "                     1    m \n",
    "      J(θ0 , θ1) = -----  ∑ (hθ(xi) - yi)² + λ |slope|\n",
    "                    2m   i=1            \n",
    "                                        [:. λ = hyper parameter ]\n",
    "                                        [:. slope = coffiescent]\n",
    "            \n",
    "            \n",
    "\n",
    "what is the Advantage os lasso regration ?\n",
    "  Ans :=\n",
    "        \"Lasso_regration\" actually hepls us to do \"feature_selection\" \n",
    "        or \"L1_regularisation\" hepls you to do something called as \n",
    "        \"feature selection\" \n",
    "        \n",
    "    how it is helps you to  do the \"feature selection\"  ?\n",
    "    \n",
    "    lets say u have hypothesis\n",
    "    \n",
    "    hθ(x)=θ0 + θ1x1 + θ2x2 + θ3x3 +......+ θnXn \n",
    "    \n",
    "    if my θ = 0   -->hθ(x) = 0\n",
    "    \n",
    "   let say in this hθ(x), θ = 0.13 has very less slop  [ :.θ= \"Slop\"]\n",
    "\n",
    "   this \"0.13\" is nothing but 1 unit movement in the \"x-axis\" with respect 1\n",
    "    unit movement in the \"y-axis\" [\"Slop\"] so acording to the \"Spearman\"(1) \n",
    "    \"Correlation\" 0.13 is very less so its not efecting the output that much\n",
    "    So when we increse \"λ\" this \"Slope\" \"Decrese\" or become 0\n",
    "\n",
    "\n",
    "    \n",
    "lets say λ = 0                  λ =10                 λ =40  \n",
    "                                                              *\n",
    "     |                     |               *    |             *\n",
    "     |                     |              *     |            *  \n",
    "     |*            *       | *           *      |*         *\n",
    "     | *          *        |  *         *       | *      *\n",
    "     |  *        *         |   *      *         |   •  * \n",
    "     |     * • *           |      •             |     \n",
    "    -|-|------------      -|--|-------------  --|-- |----------- \n",
    "     | 0                   |  0                 |   0\n",
    "        \n",
    "        \n",
    "see here \"Global minima\" going to 0 if i increase \"λ\" value\n",
    "\n",
    "    \n",
    "    so in \"Losso_Rigretion\" where ever the \"Coefficient\"/\"Slop\" near\n",
    "    to 0 or less, \"Losso\" is going to \"bring them\" to the 0 \"Slop\"/\"Coefficient\"\n",
    "    \n",
    "    that basically means if i have \"unssessary_feature\" it will actually\n",
    "    hepls us to do \"Feature_Selection\",\n",
    "    \n",
    "                                                                 \n",
    "        |                 *\n",
    "        |*             *  \n",
    "        | *         *\n",
    "        |    •  *             here my\"Global minima\" come to 0, so when \n",
    "        |                    your actually getting 0 that basically means \n",
    "        |                     your making \"Slop\"/θ1/cofficient is/goes 0\n",
    "      --|-|--|-----------   :.so which are not \"Correlated\", that will actully\n",
    "        |-1  0               become 0  \n",
    "    \n",
    "    \n",
    "    so when my \"λ\" increasing my \"slope\" which is not at all \"corelated\"\n",
    "    so it will become 0 its aproctumatly equal to 0, so this where the entire\n",
    "    feature(\"Cost_function\"=0) will be neglacted this turm is nothing but\n",
    "    \"Feature_selection\" .\n",
    "    \n",
    "    ** final Conclussion  ** \n",
    "    \n",
    "      Ridge := \"Reduce will Overfitting\" \n",
    "      Lasso := helps in \"Feature selection\"    \n",
    "        \n",
    "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "            \n",
    "when should we use \"Ridge\" and \"Lasso\" and \"Elastic_net\" ?\n",
    "\n",
    " Ans=\n",
    "    \n",
    "what is \"Multy_Collinearity\"?\n",
    "    @_Multycolinearrity :=\n",
    "    \n",
    "    I have dataset like \n",
    "    \n",
    "    f1   f2     f3    output  --> f1,f2,f3 are independent features\n",
    "    \n",
    "\n",
    "lets say if this(f1,f2) features directly \"correlated\" like 99% to the\n",
    "\"output_feature\" we can either \"remove\" one feature, so whenever you have \n",
    "this kinda condition you can use \"Ridge_regration\" that is your \"L2_regularisation\" \n",
    "\n",
    "\n",
    "Whenever you think you have \"so many features\" lets say i have\n",
    "500 600 \"features\" i dont want to use another tecnique of doing\n",
    "\"Feature_Selection\" can diretly use \"Lasso_regration\" that is\n",
    "your \"L1_regularisation\"   \n",
    "\n",
    "when you have 500,600 features you may have \"Multy_Collinearity\" \n",
    "means you may have correlation(సంబంధం) B/w features\n",
    "\n",
    "\n",
    "if you want to use \"Ridge\" and \"Lasso\" both you can use \n",
    "\"Elastic_net\" Regration "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1849c32",
   "metadata": {},
   "source": [
    "# 3. Elastic_Net Regration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ad67a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Elastic_Net Regration \n",
    "\n",
    "\n",
    "here your \"Cost_function\" will be having both the components \n",
    "\n",
    "    go back to cost_function \n",
    "    \n",
    "   Cost_function :=\n",
    "                   1    m \n",
    "    J(θ0 , θ1) = -----  ∑ (hθ(xi) - yi)²\n",
    "                  2m   i=1\n",
    "        \n",
    "        \n",
    "    if we Combine Redge and Losso_regration it will becoe an \n",
    "    \"Elastic_Net\" Regration\n",
    "    \n",
    "                   1    m \n",
    "    J(θ0 , θ1) = -----  ∑ (hθ(xi) - yi)² + λ1 (slope)² + λ2|slope|\n",
    "                  2m   i=1           \n",
    "                                        [:. λ = hyper parameter ]\n",
    "                                        [:. slope = coffiescent]\n",
    "                \n",
    "    \"λ1\" and \"λ2\" are saperate values.\n",
    "            \n",
    "in this Regration we are using both componants so here we are also\n",
    "redusing \"Over_fitting\" and we also performing \"Feature Selection\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ab7236",
   "metadata": {},
   "source": [
    "# 4. Logistic Regration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c74aa81",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (conda_tmp/ipykernel_12172/1463922599.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\conda_tmp/ipykernel_12172/1463922599.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    4. Logistic Regration :={\"Classification Problem \"}\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "4.Logistic_Regration :={\"Classification Problem \"}\n",
    "    \n",
    "    this is the first \"Superwised Machine Lerning algorithem\" with respect\n",
    "    to  \"Classification\" this is specifically related to a \"Binary classification\"\n",
    "    problem.\n",
    "    \n",
    "    here bassically\n",
    "    \n",
    "    we try to find out \"best fit line\"\n",
    " \n",
    "   NOTE imp :=\n",
    "        in \"Linear\" and \"Logistic\" what kind of \"regularisation\"\n",
    "        you use ?\n",
    "           ANS :=\n",
    "                we use \"L1 L2 Elastic_Net Hinge Loss\"\n",
    "\n",
    " \n",
    "    Ex:= dataset like \n",
    "        \n",
    "no.of              no.of             no.of         oput\n",
    "play hours       sleep hours      study hour    pass/faile\n",
    "   7hr               6hr              1hr          fail\n",
    "   2hr               6hr              5hr          pass    \n",
    "                                            here \"dependent_feature\" have 2 outputs\n",
    "                                            so this is \"binari classificattion\"\n",
    "when to use \"Logistic_Regration\"?\n",
    "\n",
    "when ever you have \"feature\" like \"fix no.of categories\" or if your feature\n",
    "have same categiries like if it has 2 same categiries or more then 2 same\n",
    "categiries we would use \"Logistic_Regration\".\n",
    "\n",
    "can we solve \"classification\" problem with the help of \"Linear/simple\"\n",
    "\"regration\" ?\n",
    "\n",
    "        trainig data\n",
    "    \n",
    "    \n",
    "    let sat my dataset like\n",
    "    \n",
    "  pass/Faile\n",
    "     Y\n",
    "     |                pass \n",
    "   1----------------*-*-*-*-*     *new data point\n",
    "     | \n",
    "     |                  |\n",
    "     |                  |     }\n",
    "     |                        } this is my \"Traning_data\"\n",
    " 0.5--------------•           }\n",
    "     |            | \n",
    "     |            |\n",
    "     |            |  \n",
    "     |faile       |     |\n",
    "  ---|-*-*-*-*-*--|-----*-----*-->X  no.study.hurs\n",
    "   0 |    <    threshhold     >\n",
    "        \n",
    "    if i study \"less then\" this \"threshhold\" i will fail and datapoints\n",
    "    foll at x-axis (<threshhold hours ) and y-axis 0\n",
    "    if i study \"grater then\" this \"threshhold\" i will pass and datapoints\n",
    "    foll at  x-axis (>threshhold hours) and  y-axis 1\n",
    "    \n",
    "now can i solve this problem with the hepl of \"simple/linear\"\n",
    "Regretion ?\n",
    "\n",
    "  in \"linear/simple\" regration we find out \"best fit line\"\n",
    "    \n",
    "if i find out the \"best fit line\" for this \n",
    "\n",
    "    pass/Faile\n",
    "     Y\n",
    "     |                /\n",
    "   1-----------*-*-*-/-*-* ---    \n",
    "     |              /\n",
    "     |-------------/*|\n",
    "     |            /  |                [🔴 = \"test_datapoint\"]\n",
    "     |           /   |\n",
    " 0.5-|----------/*|  |\n",
    "     |         /  |  |\n",
    "     |--------/*  |  |\n",
    "     |       / |  |  |  \n",
    "     |      /  |  |  | \n",
    "  ---|-*-*-/*--⚫-|-🔴-----*-->X  no.study.hurs\n",
    "   0 |    /       |       \n",
    "                  |\n",
    "              \"threshhold\"    \n",
    "    \n",
    "    its my ⚫ new \"data point\" if it is \"less then\" < 0.5\n",
    "    then it will become 0 means \"fail\"\n",
    "        \n",
    "    supose if i have new \"data point\" 🔴 there if it is\n",
    "    \"greter then\" > 0.5 then i am basically going to get\n",
    "    1 means \"pass\"\n",
    "                \n",
    "so here with respect \"new/any data points\" i will be able to get the\n",
    "\"output\".\n",
    "                \n",
    "so yes we are able to solve \"classiication\" problem with the help of \n",
    "\"simple/linear_regretion\",\n",
    "\n",
    "but why we actually need this \"Logistic_Regration\" ?\n",
    "\n",
    "let say if my dataset has new \"data point\" (\"Outliers\")\n",
    "in my \"Training_data\" set my data point i have like \"Outliers\"\n",
    "b coz of that Outliers my \"best fit line\" looks like below\n",
    "    \n",
    "    pass/Faile                   🔴 = Outlier datapoint\n",
    "     Y                /\n",
    "     |               /\n",
    "   1----*-*-*-*---  / 🔴   \n",
    "     |             /           [• = \"Test_data_point\"]\n",
    "     |            /           \n",
    "     |           /         \n",
    " 0.5-|---       / \n",
    "     |---------/*|\n",
    "     |        /  |          \n",
    "     |       /   |          \n",
    "  ---|-*-*--/*-|-•---------->X  no.study.hurs\n",
    "   0 |     /  thresh\n",
    "          /    hold \n",
    "         \n",
    "\n",
    "    { • } when we get new \"data point\" after the \"threshhold\" \n",
    "    it will becoming < 0.5 means \"fail\" but the is truth it \n",
    "    had to be 1 means pass, b-coz of \"Outliers\" the predition\n",
    "    is to be \"fail\". so definatly we dont need to use \"simple/linear\"\n",
    "    Regretion, this is \"1 problem \"\n",
    "    \n",
    "and 2nd problem is that\n",
    "\n",
    "    pass/Faile \n",
    "\n",
    "     Y\n",
    "     ^                    /\n",
    "     |                   /\n",
    "     |------------------/*                 \n",
    "     |                 / |        🔴 = Outlier datapoint\n",
    "     |                /  |\n",
    "     |               /   |        [🟡 = new\"Test_data_point\"]\n",
    "   1----*-*-*-*---  /🔴  |\n",
    "     |             /     |\n",
    "     |            /      |    \n",
    "     |           /       | \n",
    " 0.5-|-         /        |\n",
    "     |---------/*|       |\n",
    "     |        /  |       |  \n",
    "     |       /   |       |  \n",
    "  ---|-*-*--/*-|-•-------🟡->X  no.study.hurs\n",
    "   0 |     /  thresh\n",
    "     |    /    hold \n",
    "     |   /\n",
    "     |  /      \n",
    "     |/    \n",
    "     / \n",
    "    /|\n",
    "   / |\n",
    "\n",
    "    \n",
    "    if i get 🟡 new \"data_point\" on x-axis if it is \n",
    "    greater then 1, my output \"should be either 0 or 1\" but\n",
    "    we are getting grater then 1 so this is a \"2nd problem\"\n",
    "    \n",
    " the scenarios like\n",
    "\n",
    " * either i am getting \"grater then\" -1 b-coz of 🟡 new datapoin\n",
    " * either i am getting \"less then\" 0  means -ve value\n",
    " * and b-coz of \"Outliers\" my entire line is getting \"shifted\"\n",
    " * with respect to \"Training_data\" obviously the \"Error\" will be\n",
    "   \"high\" so it may become \"Underfitiing\".\n",
    "    \n",
    "  yes we can remove \"Outliers\" but we are having \"multyple\"\n",
    "\"problems\", here 1st is with respect to \"Output/dependent\" feature \n",
    "values it should be 1 or 0 if it greter then 1 that is problem\n",
    "if it is -ve/<0 that is also a problem ,2nd if it has \"Outliers\"\n",
    "there is chances for the right \"test_data_point\" will also it become\n",
    "\"fail\" b-coz of \"outliers\"\n",
    "\n",
    "we bring some \"component\" so that we can \"squash\" this into\n",
    "\"Sraight line\", also make sure that take of \"Outliers\"\n",
    "\n",
    "Now how do we solve this \"problem\" or how do we make sure \n",
    "that we  \"squash\" this \"Sraight line\" into a better way\n",
    "  \n",
    "    \n",
    "    our main aim is that \n",
    "    \n",
    "    \n",
    "       |           /-•-•-•- -> from here it shoud get \"squashed\"    \n",
    "       |   *  *   /  *\n",
    "       | *  * *  / * * *   after \"squashed\" datapoints we \n",
    "       |    *   /  *  *    will able to \"saperate\" the point\n",
    "       |  *    / *  * *    with the \"best fit line\"\n",
    "       |      /\n",
    "     --|-•-•-/----------   x-axis\n",
    "       |  |    \n",
    "          |\n",
    "        from here it \n",
    "        shoud get \"squashed\"\n",
    "        \n",
    "        \n",
    "       my hypothesis will hθ(x) = θ0 + θ1x b-coz i need\n",
    "    to Create the \"best fit line\"\n",
    "        \n",
    "    Now my   hθ(x) for \"Logistic regration\"\n",
    "        will be a function of ---------> hθ(x) = g( θ0 + θ1x1 )\n",
    "        \n",
    "       --> hθ(x) = g( θ0 + θ1x1 )  :.θ0 + θ1x1 for 2 \"cordinates\"\n",
    "                                     if i have more then 2 \"cordinates\"\n",
    "                                     i can write my eqn as \n",
    "                           hθ(x) = θ0 + θ1x1 +θ2x2+θ3x3 +..+θnXn\n",
    "                    \n",
    "    here what this \"g \" ?\n",
    "    \n",
    "                         \n",
    "       --> let say Z = θ0 + θ1x1 \n",
    "    \n",
    "                       :. g is a function on \"Z\"\n",
    "        hθ(x) =g(Z)  [:. g is a \"activation function\" ]\n",
    "        \n",
    "what this \"activation function\"  does ?\n",
    "ans :=\n",
    "    \n",
    "    Instead of writing \"g\" in that place we wrire \"sigmoied activation\"\n",
    "    function 1 / 1+(e)-z\n",
    "    \n",
    "                 1\n",
    "     hθ(x) = ---------  this \"activation function\" does 2 things\n",
    "              1 + (e)-z   \n",
    "        \n",
    "    *1 thing is that it will help doing the \"squashing\" \n",
    "        \n",
    "why it will helps doing this \"squashing\" ?\n",
    "  b-coz whatever value you apply over here it converts bitween [ 0 to 1 ],\n",
    "  which is called as \"sigmoied activation\" function , it will also used in \"Deep Learning\"\n",
    "        \n",
    "        so \"Sigmoied activation\" function it will converts the value\n",
    "        b/w [ 0 to 1 ]\n",
    "        \n",
    "        \n",
    "how this \"sigmoied activation\" looks ?\n",
    "Ans:-\n",
    "    with respect to different different \"Z\" value \n",
    "    it looks the \"curv\" like\n",
    "        \n",
    "                                 \n",
    "                           1|     * * *\n",
    "                            |  *     \n",
    "                            | * \n",
    "                            *--0.5   \n",
    "                           *|    \n",
    "                  *  *  *   |\n",
    "                            |\n",
    "             --|---|---|----|---|---|---|---Z \n",
    "              -3  -2  -1   0|   1   2   3 \n",
    "                            |       \n",
    "        \n",
    "    with respect to \"z\"   when ever this values are +ve it\n",
    "    will be ranging B/W [0 - 1]\n",
    "    \n",
    "    when ever this values are -ve it will be ploting with \n",
    "    respect to \"y\"\n",
    "        \n",
    "        \n",
    "if i talk about  my hθ(x) is nothing but\n",
    "\n",
    " Logistic regration \"hypothesis testing\" will be :=\n",
    "            \n",
    "                        1            }\n",
    "        hθ(x) =   ---------------    } this entire Equation is ranging \n",
    "                  1 + e-(θ0+θ1x1)    }  b/w [0 - 1]\n",
    "                \n",
    "               this is  how i find out my \"best fit line\" along\n",
    "                with \"Squashing\"\n",
    "\n",
    " [:. e = exponential ,\"Squashing\" means cutting this line not \"extending\"]\n",
    "        \n",
    "    why we are using \"e\" = \"exponential\" thing is that \n",
    "    b-coz we want the value B/W [ 0 - 1 ].\n",
    "    \n",
    "         \n",
    "\n",
    "\"linear regration hypothesis\":=\n",
    "    hθ(x) = θ0 + θ1x1\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "what is our \"main aim\" ?\n",
    "Ans:-\n",
    "\n",
    "let say \"Training_data\" set format will be like\n",
    "\n",
    "{(x1 ,y1),(x2,y2),(x3, y3)......(xn,yn)}\n",
    "\n",
    "   my \"y\" = should be {0 or 1} i specifically have \"2 outputs\"\n",
    "                              this is a \"binary classification\"\n",
    "        \n",
    "                 1\n",
    "     hθ(z) = ---------   Z = θ0 + θ1x1   \n",
    "              1 + e-z\n",
    "        \n",
    "        if  θ0  = 0 then it indicate \"intercept\" = 0\n",
    "        \n",
    "        \"intercept\" = 0 that basically means the \"line\" it \n",
    "        \"passes\" through the \"origin\" \n",
    "                     |  /\n",
    "                     | /\n",
    "                  ---|/-------\n",
    "                    0|\n",
    "      \n",
    "now our \"main aim\" to change \"θ1\" try to find out the \"best fit line\" \n",
    "such that  it 'classifies'/\"Saperates\" all the \"data points\"\n",
    "        \n",
    "\"Cost_function\" :=\n",
    "    cost function of linear Regression :=\n",
    "    \n",
    "                   1    m \n",
    "    J(θ0 , θ1) = -----  ∑ (hθ(xi) - yi)²\n",
    "                  2m   i=1\n",
    "        \n",
    "        if \"intersept\" is θ = 0 \n",
    "        \n",
    "                   1    m \n",
    "        J( θ1) = -----  ∑ (hθ(xi) - yi)²\n",
    "                   2m  i=1\n",
    "        \n",
    "    if \"intersept\" is θ = 0 and my \"hypothesis\" will be \n",
    "                 1\n",
    "    hθ(x) = ------------   this will be my \"Opt\" means \"^y\" :.Predicted point\n",
    "             1 + e-(θ1x)  ---> can i use this \"hypothesis\"\n",
    "                              in my \"Cost_function\"\n",
    "            \n",
    " so the question is that can i use this \"hypothesis\"\n",
    "in my \"Cost_function\" ?\n",
    "\n",
    "at the end of the day \"Gradient decent\" / \"Parabola\"\n",
    "will get created, as a Cost_function with respect \"Slope\"\n",
    "and \"Cost_function\" itself\n",
    "   \n",
    "so this \"hypothesis\" equation we can not directly use in \"Cost_function\" \n",
    "the ression is that u need to understand 2 things \n",
    "\n",
    " 1. NON Convex function\n",
    " 2. Convex function \n",
    "\n",
    "what is Convex function  ?\n",
    "\n",
    "when ever we constract \"Mean Squared Error\"[MSE] with respect to\n",
    "\"hypothesis\" we will get our \"Cost_function\" will be getting\n",
    "like this \"Curv\"\n",
    "\n",
    "                  |\n",
    "                  |*                *\n",
    "                  |*               *\n",
    "                  | *             *     this is \"Gradient_Decent\"\n",
    "                  |  *           *   \n",
    "                  |     *     *\n",
    "             -----|--------🔹-------------- \n",
    "                            \n",
    "                      🔹= this point is my \"Global minima\" \n",
    "                    \n",
    " \n",
    "\n",
    " now when ever we get this type of \"Gradient_Decent\"/\"Parabola\" \"Curv\" \n",
    "we basically say it as \"Convex function\" \n",
    "   \n",
    "    Quadratic_Equation  = ax² + bx + c =0  \n",
    "    \n",
    "the  \"Quadratic_Equation\" it usually form \"Curv\" like \"Parabola\"\n",
    "or \"Gradient_Decent\" Curv\n",
    "\n",
    "what is NON Convex function ?\n",
    "\n",
    "  b-coz of the \"usage\" of the \"Sigmoied_activation\" we will not get the \"Curv\"\n",
    "like \"Gradient_Decent\"/\"Parabola\" this is nothing but \"Non Convex Function\"\n",
    "instead we get \"Curv\" looks like\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "       |\n",
    "       |   *        \n",
    "       |   *     * *      * *              * *       * *        *\n",
    "       |     *🔻*  *     *  *             *   *     *   *     * \n",
    "       |             *🔻*    *           *     *   *    ---🔻---- \"slop\" is 0\n",
    "       |                       *        *        🔻              b-coz its Strait \n",
    "       |                         *    *                          line\n",
    "     --|---------------------------🔹-------------------------------->\n",
    "    \n",
    "    \n",
    "        🔹  =  this loest point is called as \"Global minima\"\n",
    "    \n",
    "        🔻 =  these all points are called as \"Local minima\"\n",
    "        \n",
    "So if you have \"so many local minima\" we will never reach\n",
    "the \"Global minima\"\n",
    "        \n",
    "    but our aim is to come over \"Global minima\" point\n",
    "        \n",
    "what happens in \"local minima\" ?\n",
    "    when ever we try to find out the \"Slop\" we create a \"Srait line\"\n",
    "   the \"Slop\" will be 0 so it may stack over there\n",
    "\n",
    "in case of \"Gradient_Decent\" we easily reach the \"Global minima\"\n",
    "with the hep of \"Slop\" lines(+ve or -ve slop) but in case of\n",
    "\"Loca minima\" we stack at \"Local minima\"(Strait line)\n",
    "    \n",
    "               1         now if i use this  \"hypothesis\" in my\n",
    "   hθ(x) = ------------  \"Cost_function\" we may usually  end up with \n",
    "           1 + e-(θ1x)   \"NON Convex function\" \n",
    "                         that is the reson we dont use this in \"Cost_function\"\n",
    "                 so we have to use another \"log_Cost_function\" or \"log_loss\" we say\n",
    "                             \n",
    "                \n",
    "                \n",
    "@why we are getting non_convex function ?\n",
    "  Ans := if your using  \"sigmoied activation\" inside my \"cost_function\" \n",
    "        that point of time if you \"plot your thita\" with respect to your \n",
    "        \"cost_function\" you will be getting \"non_convex function\"\n",
    "            \n",
    "Now what \"Log_loss\" or \"log_Cost_function\" says?\n",
    "\n",
    "V.V.IMP  \n",
    "     \n",
    "\n",
    "*Logistic_Regration Cost_function (log loss) :=\n",
    "    \n",
    "\n",
    "why we are using \"log loss\"/\"Log_Cost_function\" ?\n",
    " becoz of  if we are going to use hθ(x) = \"sigmoid_activation_function\"\n",
    "inside \"cost_function\" it may end up with \"NON Convex function\"\n",
    "it has \"Curv\" like in that so many \"local minimas\" are there it is\n",
    "dificult to reach the \"Global minima\"\n",
    "so that is the ression we want to go with \"log loss\"/\"Log_Cost_function\"\n",
    "it plot like \"Gradient_decent\" so its easy to reach the \"Global minima\" \n",
    "   \n",
    "    \n",
    "    \n",
    "         |   -log(hθ(x))  use this \"cost_function\" if your value  y=1\n",
    "  j(θ1)= | \n",
    "         |   -log(1-hθ(x)) use this \"cost_function\" if your value y=0  \n",
    "\n",
    "    \n",
    "    :. hθ(x) = \"^y\" my \"predecte point\"\n",
    "        \n",
    "this \"Logistic_Regration_Cost_function/Log_loss\" \n",
    "always give you \"Gradient descent\" curv and having 1 \"Global minima\"\n",
    "\n",
    "                                       |*           *\n",
    "                                       | *         *\n",
    "                                       |   *      *\n",
    "                                       |......🔹.........\n",
    "    \n",
    "    internally i can write this as \n",
    "\n",
    "    J(θ0,θ1) = -y log(hθ(xi)) - (1-y)log(1 - hθ(xi))  \n",
    "    \n",
    "    if y = 1 i get = -log(hθ(x)) this equation\n",
    "    if y = 0 i get = -log(1-hθ(x)) this equation\n",
    "    \n",
    "        \n",
    "            here \"y\"- is basically means the \"Output/dependets feature\"       \n",
    "    \n",
    "over all if i go and write Cost_function\n",
    "\n",
    "this becomes my \"Cost_function\" for \"Logistic_Regration\" :=\n",
    "----------------------------------------------------\n",
    "\"cost_function\"/\"log_Cost_function\":=\n",
    "                   1    m \n",
    "    J(θ0 , θ1) = -----  ∑ (hθ(xi) - yi)²\n",
    "                  2m   i=1\n",
    "        \n",
    "         - 1    m \n",
    " J(θ1) = -----  ∑ [yi log(hθ(xi)) + (1-yi)log(1-hθ(xi))]\n",
    "          2m   i=1\n",
    "    \n",
    "    :.  assume \"θ0\" is passing  through letter on we can make it\n",
    "        2 or 3 dimention and all\n",
    "    \n",
    "\"Cost_function\" with respect \"logistic regration\" \n",
    "    \n",
    "                  1\n",
    "    hθ(xi) = -----------  ===> \"Hypothesis\"\n",
    "             1 + e-(θ1x1)\n",
    "        \n",
    "That bassically means we rae continous changing the \"θ1\" value unless and\n",
    "untill we Reduse the \"loss/log/Cost_fun\" to reach the \"Global minima\"\n",
    "        \n",
    "@Gradient Desent uses Convergence Algorithem:=\n",
    "    \n",
    "Now you also need to write \"Convergense_algorithem\"\n",
    "     Ans :=\n",
    "    convergence algrm says repeat until \"Convergence\"\n",
    "     here we need to keep update \"θ\" value to reach the \"Global minima\"\n",
    "        \n",
    "           {  \n",
    "                             ∂(J(θold))\n",
    "            θnew = θold - α -----------  ==> coefficient/Slop updation  \n",
    "                               ∂θold\n",
    "           } \n",
    "            \n",
    "        here similarly it bcomes\n",
    "           {  \n",
    "                             ∂(J(θj))\n",
    "            θj ≈ θj - α -----------  ==> coefficient/Slop updation  \n",
    "                               ∂θj\n",
    "           }  \n",
    "     \n",
    "\n",
    "\n",
    "         - 1    m \n",
    "J( θ1) = -----  ∑ [yi log(hθ(xi)) + (1-yi) log(1-hθ(xi))] + λ1(slope)²+λ2|slope|\n",
    "          2m   i=1   \n",
    "    \n",
    "\n",
    "@Non convex function disadvantages \n",
    "\n",
    "  * it has many \"local minima point\"\n",
    "\n",
    "@advantages of Convex_function Quadratic Equation\n",
    "  \n",
    "  * one global minima    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3289aba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03125"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78451c2",
   "metadata": {},
   "source": [
    "## performance matrix (Binary classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512c1e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "% performance matrix (Binary classification)\n",
    "\n",
    "i have data set like below\n",
    "               o/p_feature\n",
    "                actuall     predicted of the Model\n",
    "    x1    x2        y        ^y         \n",
    "    -     -         0         1 \n",
    "    -     -         1         1\n",
    "    -     -         0         0\n",
    "    -     -         1         1  \n",
    "    -     -         1         1\n",
    "    -     -         0         1\n",
    "    -     -         1         0\n",
    "    \n",
    "    yes by seeing it its a \"Binary Classification\" problem b-coz\n",
    "    the Otuput_feature is having 2 \"categories\" ( 1 and 0 )\n",
    "\n",
    "    \n",
    "Now how do i check wather my Model is given a good Accuracy or not?\n",
    "Ans:=\n",
    "    with the help of \"performance matrics\" of this specific model.\n",
    "    \n",
    "    and how to find out \"Performance Matricx\" with respect this\n",
    "    Classification Problem ?\n",
    "    \n",
    "    \n",
    "                                    \n",
    "                                   on top of this\n",
    "                         1      0  here we put what are passible \n",
    "\"confussion matrix\"   |------|-----| \"actual values\" either 1 or 0\n",
    "                    1 |  3   |  2  |  \n",
    "                      |------|-----|\n",
    "                    0 |  1   |  1  |\n",
    "    on left handside  |------|-----|\n",
    "    we put \"predicted values\"  \n",
    "    for this proble we get \n",
    "    2 scenaois 1 or 0\n",
    "    \n",
    "    Now if i draw \"confusson matrix\" with respect \"actual/output\" values\n",
    "    and \"predicted\" values\n",
    "    \n",
    "                                    {actual values}\n",
    "                         1      0\n",
    "    confusson matrix  |------|-----| \n",
    "                    1 |  TP  |  FP |\n",
    "                      |------|-----|   TP & TN currect prediction \n",
    "                    0 | FN   |  TN |\n",
    "                      |------|-----|   FP & FN is my \"error turms\"\n",
    "        {predicted values}  \n",
    "        \n",
    "so with the help of datset \"TP\" and \"TN\" are correct predictions\n",
    "so to check the \"accuracy\" of Model\n",
    "            \n",
    "1 \"Accuracy\"\n",
    "                            TP + TN          3+1        4 \n",
    "\"Accuracy\" of the model = ------------  = --------- = ---- = 57%\n",
    "                           TP+FP+FN+TN     3+2+1+1      7\n",
    "            \n",
    "            \n",
    "so here Model \"Accuracy\" = 57%\n",
    "\n",
    "always we need to remember when ever we create a \"model\" we\n",
    "should be increase \"TP\" and \"TN\" and we shold be decrease\n",
    "\"FP\" and \"FN\", b-coz this(\"FP\" and \"FN\") are the Error turms\n",
    "\n",
    "\n",
    "\n",
    "            i have \"training dataset\" like\n",
    "            \n",
    "        0 ------> 900 data points  }\n",
    "        1 ------> 100 data points  } ---> this is imbalance dataset\n",
    "        \n",
    "        0 ------> 600  data points }\n",
    "        1 ------> 400 data points  } ---> this is balance dataset\n",
    "        \n",
    " \n",
    "Precission\n",
    "Recall\n",
    "Fbeta  _score\n",
    "        \n",
    "1 .\"Precission\" :=\n",
    "    \n",
    "    out of all \"predicted output\" how many of them are been predicted \n",
    "correctly or as \"TP\" are there\n",
    "formula:=\n",
    "    \n",
    "        TP\n",
    "    = -------\n",
    "       TP+FP            \n",
    "    \n",
    "    \n",
    "2 . \"Recall\" :=     \n",
    "    \n",
    "   out of all the \"actuall values\" how many them are \n",
    "were as \"TP\"   this also called as \"TP rate\"  \n",
    "\n",
    "        TP\n",
    "    = -------\n",
    "       TP+FN\n",
    "        \n",
    "Ex :=   \n",
    "     whether the person has cancer or not\n",
    "     the person actually having cancer but my model says \n",
    "     not having cancer  so here we focuss on reducing\n",
    "     \"false nagetive\" so for this i am going use \"Recall\"\n",
    "\n",
    "                         1      0\n",
    "    confusson matrix  |------|-----| \n",
    "                    1 |  TP  |  FP |\n",
    "                      |------|-----|   TP & TN currect prediction \n",
    "                    0 | FN   |  TN |\n",
    "                      |------|-----|  \n",
    "    \n",
    "\n",
    "3.\"F_Score\" /\"F_Beta\" Score  :=\n",
    "    we basically use this \"F_Score\" when we have to concider both the\n",
    "    resons \"FP\" and \"FN\"\n",
    "    \n",
    "\n",
    " ley say i want to develop my \"ML\" model for that  \n",
    "\n",
    "Example 1 :=\n",
    "    \n",
    "Email Spam claciffication :=\n",
    "     in spam classification whether should i focuss redusing \n",
    "    \"false possitive\" are wether i focuss \"false nagetve\"\n",
    "    \n",
    "    \n",
    "Ex:=  if there is not \"spam\"(AV valu=0) mail, but model perdict(PV val=1)\n",
    "    as there is have some spam mail,so we need to reduse \"FP\"(1) [:. PV val 1 come]\n",
    "    in order to do this we focus on redusing \"FP\" false +ve we need to focus\n",
    "    on \"Pression\" b-coz i have \"FP\" in \"Pression\"formula\n",
    "    \n",
    "                                                  TP\n",
    "                                     Pression =  -----\n",
    "                                                 TP+FP\n",
    "            \n",
    "whethe the person has Canser or not ?\n",
    "let say the person Actually having a Cancer My Model predicts like\n",
    "the person is not having cancer\n",
    "\n",
    "so here if \"actual value\" is = 1 but the \"predicted is\" = 0\n",
    "so in this order we need to focuss on increase \"FN\" so for this\n",
    "i am going to use \"Recall\" b-coz i have \"FN\" in this\n",
    "                        \n",
    "                         TP\n",
    "            \"Recall\" = -------\n",
    "                        TP+FN\n",
    "                \n",
    "        with the help of hyper parameter tuning i have \"reduce\"\n",
    "        or \"increse\" (FP and FN)\n",
    "        \n",
    " \n",
    "                         1      0\n",
    "    confusson matrix  |------|-----| \n",
    "                    1 |  TP  |  FP |\n",
    "                      |------|-----|   TP & TN currect prediction \n",
    "                    0 | FN   |  TN |\n",
    "                      |------|-----|\n",
    "\n",
    "Example 1 :=\n",
    "    \n",
    "    tomorrow stack market gonna crash \n",
    "    to focuss on \"FP\" (we need to reduse)\n",
    "    \n",
    "    for this problem statement u need to undestand 2 things\n",
    "    whether we are creating a \"model\" for the \"company\" or for\n",
    "    the \"people\",\n",
    "    \n",
    "    if you creating the \"model\" for the \"people\" u focuss on \"FN\"\n",
    "    for company you focuss on \"FP\"\n",
    "    \n",
    "    there will be some scenarios need to focuss also on redusing\n",
    "    both \"FN\" and \"FP\" this will be possible by a technique called \n",
    "    as \"F_Beta_Score\".\n",
    "    \n",
    "    \n",
    "                                     precision * recall\n",
    "\"F_Beta\" Score formula == (1 + β²) -----------------------\n",
    "                                   β² *[precision+recall]\n",
    "    \n",
    " 1   let say β = 1 that bassically means \"FP\" \"FN\" both are imp\n",
    "    \n",
    "          precision * recall\n",
    "     (1+1)--------------------  ===> this is \"Harmonic mean\"\n",
    "          precision+recall\n",
    "        \n",
    "        if  β = 1 \"Fbeta_score\" become \"Harmonic mean\"\n",
    "        \n",
    "        \n",
    "    β  = its a \"parameter\" ,and we can take value \"Dynamically\"\n",
    "        with respect to problem stmt\n",
    "            \n",
    "    if \"FP\" and \"FN\" are Imp then \"β\" = 1\n",
    "        \n",
    "  2   lets say \"FP\" is more imp then \"FN\"(FP>>FN) then we\n",
    "      have to reduse my   \"β\" \n",
    "    \n",
    "      so \"β\" = 0.5\n",
    "    \n",
    "                    precision * recall\n",
    "        = (1 + 0.25)----------------------- this equation for when my\n",
    "                    [0.25][precision+recall]  \"FP\" is more IMP then \"FN\"\n",
    "                                              (FP>>FN)\n",
    "            in this case we focuss/use on \"Pression\"\n",
    "                \n",
    " 3   lets say \"FN\" is more imp then \"FP\"(FP<<FN) so \"increase\"\n",
    "      the β = 2\n",
    "    \n",
    "                        precision * recall     \n",
    "              = (1 + 4)----------------------this equation for when my\n",
    "                       [4][precision+recall]  \"FN\" is more imp then \"FP\"\n",
    "                                              (FP<<FN)\n",
    "                    \n",
    "                in this case we focuss/use on \"Recall\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0036699c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19186af4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
